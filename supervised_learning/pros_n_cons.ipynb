{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Natural Handling data of mixed type | Naturally handle missing values | Insensitive to monotone transformation of inputs | Computational Scalability (large $N$) | Intrinsic Feature Selection | Interpretability |\n",
    "|-----------|--------------------------|-----------------------|---------------|-----------------|--------------------------------------|----------------|\n",
    "|Neural Nets|Poor. But feature encoding can be viewed as part of neural net|Poor|Poor. This is why BatchNorm is useful|Poor|Poor|Poor|\n",
    "|SVM|Poor|Poor|Poor|Poor. Usually good for small- and medium-sized problems|Poor. SVM can be adversely impacted by noisy dimensions|Fair. If one can make use of the support vectors|\n",
    "|MARS|Good.Recall that binges are added one point at a time|Poor (not understand why ESL thinks it is good)|Poor|Good|Good. Again recall that hinges are added one input at a time and also provide feature importance|Good. Can be compared to linear regression in this regard|\n",
    "|kNN|Poor|Poor (not understanding why ESL says otherwise)|Poor. kNN is extremely sensitive to how distance is defined|Poor|Nope|Fair, if you use the neighboring instances to justify|\n",
    "|Naive Bayes|Good. Due to the independence assumption, each dimension is handled separately|Poor|Good. The parametric assumption, such as those in the Gaussian NB, should naturally handle this|Good. NB is known for fast computation|Nope|Poor, especially in high dimension|\n",
    "|Logistic Regression|Poor|Poor|In some sense yes. But a totally different story with regularizations|Fair|Fair. But suffers from colinearity|Good. And there are confident intervals about coefficients|\n",
    "|Linear and Quadratic Discriminat Analysis|Poor|Poor|Yes - the estimated parameters in the model should take care of that|Closed-form solution. Very fast in computation|Poor. Suffers from colinearity|Fair: only good when there is obvious ellipsoid clusters|\n",
    "|Linear Regression and related models|Poor|Poor|Yes - unless there is regularization|Closed-form solution, very fast in computation though|Poor. Suffers from colinearity|Good. And there are confidence intervals about coefficients|\n",
    "|CART and Related Model|Good|Good. Probably the only common model to have this ability|Good|For single trees, the greedy algo can be very fast. Other extension algo might suffer for speed, though some (except boosting) can be parallelized|Good. Bagging even deal with co-adaptation head-on|Single tree is good, but all other extensions sacrifice that for accuracy|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | Robustness to Outliers | Predictive Powers | Can do online learning | Memory Efficient | Ability to extract linear relationship | Applicable in $p>>N$ |\n",
    "|-----------|--------------------------|-----------------------|---------------|-----------------|--------------------------------------|----------------|\n",
    "| Neural Nets | Poor | Good | Yes | Poor - unless you have a small neural net | Fair. But may struggle to learn difference | Yes |\n",
    "| SVM | Fair. Only support vector determines boundary | Good | Yes, when the optimizer uses SGD rather than QP applied on the daul problem | Good | Yes | Yes. Sometimes unregulated SVM often works as well as the best regularized ones | \n",
    "| MARS | Poor. In particular, the knot point in the basis functions will be impacted | Good | Yes. Just probably continue to add/trim some basis functions with new data | Good. Only need to save the basis functions | Fair. Probably will need to allow for linear features | Yes. MARS would choose a subset of inputs anyways, and has the reputation to perform well in high dimensions | \n",
    "| kNN | Fair, if you take medium rather than mean in regression; in classification, the majority vote is very robust to outliers | Good. In classifications, kNN can produce irregular boundaries | No | No. kNN needs to carry the whole training set | Naive Bayes | Depends on the parametric assumption | Fair, despite the simplistic independence assumption | No | Yes. This is a parametric model| (Not applicable - this is a classification model) | Yes. NB can tackle high-dimension problems efficiently|\n",
    "| Logistic Regression | Poor | Poor. It produces linear, or rigid decision boundary | Yes | Yes | Yes, it is designed to do that | Only with regularization | Poor. Can only produces linear or quadratic boundary | No. But closed-form makes batch-learning no different | Yes. But it may be a pain when $p>>N$ | Yes, by design | Only the regularized version does |\n",
    "| Linear Regression and Related Models | Poor. Robust regression procedures may improve on this | Poor. Usually considered to be high bias, but sometimes can outperforms fancier models with limited data | Yes, if it is using SGD optimizer | Yes. Only coefficients need to be saved. Not true for LWLR | Yes. That is what the model is designed to do | Only possible with regularization, or PCR/PLS |\n",
    "| CART and Related Models | All tree-based models are good, except for AdaBoost with exponential loss functions | General Tree Boosting can be compared to neural nets | Yes. Just continue to split or add more trees | Relatively good, though many trees can be intense | Poor - trees are not designed to capture linear structure | Yes. With the dealing with co-adaptation in bagging, it is even better |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- ESL, Section 10.7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
