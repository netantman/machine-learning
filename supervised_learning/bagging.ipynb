{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification\n",
    "\n",
    "Bagging or bootstrap aggregation averages the prediction over a collection of bootstrap samples with replacements.\n",
    "$$\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^B \\hat{f}^{*b}(x)$$\n",
    "* The above is a Monte Carlo estimate of the empirical distribution of the samples.\n",
    "* Note that the bagged estimate $\\hat{f}_{bag}(x)$ only differ from the original estimate $\\hat{f}(x)$ (as $B\\rightarrow\\infty$) only when the estimator is a nonlinear or adaptive function of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants and Generalizations\n",
    "\n",
    "An alternative bagging strategy is to average the class probabilities themselves, before reaching the verdict. Not only does this produce improved estimates of the class probs, but it also tends to produce bagged classifiers with lower variance, especially for small $B$ (Though this is intuitive, ESL does not provide reference, or whether this is a theoretical or empirical result)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "- Bagging are used as a way to reduce variance of the base estimator (e.g. decision trees). In particular, the intuition that bagging reduces variance or instability (see [CART](CART.ipynb)) is that each base tree may use different variable to split due to the randomized bagging samples. When subsequently averaged, the dominance of one data point to the subsequent splits is significantly reduced.\n",
    "- In many cases, bagging is very simple to implement without interfering with the inner-working of the base estimators.\n",
    "- Bagging produces Out-Of-Bag samples, which natually incorporate LOO-type CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "\n",
    "- Interpretability is sacraficed.\n",
    "- **Independence among base estimator is crucial for the success of bagging.** When predictors have high correlations, the bagged sample tend to have high correlations, affecting the effectiveness of bagging to reduce variance by taking average. This is something that [random forest](random_forest.ipynb) tries to alleviate.\n",
    "- **Error rate of the base learner still matters.** Actually, bagging a 'bad' classifier can make it worse. Simple example. Suppose $Y=1$ for all $x$, and the classifier $\\hat{G}(x)$ predicts $Y=1$ for all $x$ with probability $0.4$ and predicts $Y=0$ with probability $0.6$. The base predictor will have $60\\%$ to of misclassification rate, but that of the bagged classifier is $100\\%$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Other Models\n",
    "\n",
    "- As a way to reduce overfitting, bagging work best with high-variance base models (e.g. fully grown decision trees without pruning), while [boosting](boosting.ipynb) tends to work best with biased base models (e.g. shallow decision trees). This has to do with the opposite goal of bagging vs. boosting: bagging works to reduce variance by taking average of unbiased base estimators, while boosting works to reduce bias by taking regularized sum of low-variance base estimators. \n",
    "\n",
    "- Boosting appears to dominate bagging on most problems, and became the preferred choice. Again, [random forest](random_forest.ipynb) seems to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details and Practical Tricks\n",
    "\n",
    "`BaggingClassier` is used as a wrapper to base estimators, to provide a meta estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "bagging = BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some commonly used inputs** (shared with other tree-typed algorithms):\n",
    "\n",
    "- **`base_estimator`**: The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.\n",
    "\n",
    "- **`n_estimators`**: The number of base estimators in the ensemble.\n",
    "\n",
    "- **`max_samples`**: The number of samples to draw from `X` to train each base estimator. If int, then draw `max_samples` samples. If float, then draw `max_samples * X.shape[0]` samples.\n",
    "\n",
    "- **`bootstrap`**: Whether samples are drawn with replacement.\n",
    "\n",
    "- **`n_jobs`**: The number of jobs to run in parallel for both fit and predict. If `-1`, then the number of jobs is set to the number of cores.\n",
    "\n",
    "- **`random_state`**: If int, `random_state` is the seed used by the random number generator; If `RandomState` instance, `random_state` is the random number generator; If `None`, the random number generator is the `RandomState` instance used by `np.random`.\n",
    "\n",
    "There are other inputs that also allow for randomized features, presumably in the fashion of random forest.\n",
    "\n",
    "Note that parallerlization by `n_jobs`, while perhaps useful for bagging here or [random forest](random_forest.ipynb) since a lot of trees are built, might not benefit building a single, big tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation, Metrics and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "- ESL, Section 8.7\n",
    "- [scikit-learn Document 1.11.1](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "### Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
