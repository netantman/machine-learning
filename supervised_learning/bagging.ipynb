{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification\n",
    "\n",
    "Bagging (short for bootstrap aggregation) averages the prediction over a collection of bootstrap samples with replacements.\n",
    "$$\\hat{f}_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^B \\hat{f}^{*b}(x)$$\n",
    "* The above is a Monte Carlo estimate of the empirical distribution of the samples.\n",
    "* Note that the bagged estimate $\\hat{f}_{bag}(x)$ only differ from the original estimate $\\hat{f}(x)$ (as $B\\rightarrow\\infty$) only when the estimator is a nonlinear or adaptive function of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants and Generalizations\n",
    "\n",
    "- An alternative bagging strategy is to average the class probabilities themselves before reaching the verdict, i.e. doing soft-voting rather than hard-voting; see a general discussion in [ensemble](../meta_learning/ensemble.ipynb). Not only does this produce improved estimates of the class probs, but it also tends to produce bagged classifiers with lower variance, especially for small $B$ (Though this is intuitive, ESL does not provide reference, or whether this is a theoretical or empirical result).\n",
    "\n",
    "- When sampling is without replacement, it is called **pasting**. Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting; but the extra diversity also means that the predictors end up being less correlated, so the ensembleâ€™s variance is reduced. Overall, bagging often results in better models, which explains why it is generally preferred. However, if you have spare time and CPU power, you can use cross-validation to evaluate both bagging and pasting and select the one that works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "- Bagging are used as a way to reduce variance of the base estimator (e.g. decision trees). In particular, the intuition that bagging reduces variance or instability (see [CART](CART.ipynb)) is that each base tree may use different variable to split due to the randomized bagging samples. When subsequently averaged, the dominance of one data point to the subsequent splits is significantly reduced.\n",
    "- In many cases, bagging is very simple to implement without interfering with the inner-working of the base estimators.\n",
    "- Bagging produces Out-Of-Bag samples, which natually incorporate LOO-type CV. Indeed, for an original sample with $N$ observations, a bootstrapped sample with the same $N$ observations will have the probability of $(1-1/N)^N\\approx e^{-1}$ not to include a particular sample, which is roughly a third of $100\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "\n",
    "- Interpretability is sacraficed.\n",
    "- **Independence among base estimator is crucial for the success of bagging**, as with any ensemble methods. When predictors have high correlations, the bagged sample tend to have high correlations, affecting the effectiveness of bagging to reduce variance by taking average. This is something that [random forest](random_forest.ipynb) tries to alleviate.\n",
    "- **Error rate of the base learner still matters.** Actually, bagging a 'bad' classifier can make it worse. Simple example. Suppose $Y=1$ for all $x$, and the classifier $\\hat{G}(x)$ predicts $Y=1$ for all $x$ with probability $0.4$ and predicts $Y=0$ with probability $0.6$. The base predictor will have $60\\%$ to of misclassification rate, but that of the bagged classifier is $100\\%$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Other Models\n",
    "\n",
    "- When both data and features are sampled, and the base estimator is a tree, it is what is called a [random forest](random_forest.ipynb). More fancy names: \n",
    "    - Sampling both training instances and features is called the **Random Patches method**. \n",
    "    - Keeping all training instances but sampling features is called the **Random Subspaces method**.\n",
    "\n",
    "- As a way to reduce overfitting, bagging work best with high-variance base models (e.g. fully grown decision trees without pruning), while [boosting](boosting.ipynb) tends to work best with biased base models (e.g. shallow decision trees). This has to do with the opposite goal of bagging vs. boosting: bagging works to reduce variance by taking average of unbiased base estimators, while boosting works to reduce bias by taking regularized sum of low-variance base estimators. \n",
    "\n",
    "- Boosting appears to dominate bagging on most problems, and became the preferred choice. Again, [random forest](random_forest.ipynb) seems to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details and Practical Tricks\n",
    "\n",
    "`BaggingClassier` is used as a wrapper to base estimators, to provide a meta estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=True)\n",
    "#bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some commonly used inputs** (shared with other tree-typed algorithms):\n",
    "\n",
    "- **`base_estimator`**: The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.\n",
    "\n",
    "- **`n_estimators`**: The number of base estimators in the ensemble.\n",
    "\n",
    "- **`max_samples`**: The number of samples to draw from `X` to train each base estimator. If int, then draw `max_samples` samples. If float, then draw `max_samples * X.shape[0]` samples.\n",
    "\n",
    "- **`bootstrap`**: Whether samples are drawn with replacement - when it is `False` it is pasting.\n",
    "\n",
    "- **`n_jobs`**: The number of jobs to run in parallel for both fit and predict. If `-1`, then the number of jobs is set to the number of cores.\n",
    "\n",
    "- **`random_state`**: If int, `random_state` is the seed used by the random number generator; If `RandomState` instance, `random_state` is the random number generator; If `None`, the random number generator is the `RandomState` instance used by `np.random`.\n",
    "\n",
    "- **`oob_score`**: whether to request an automatic oob evaluation after training. The resulting evaluation score is available through the `oob_score_` variable of the bagging class instance. The oob decision function for each training instance is also available through the `oob_decision_function_` variable. In the case where the base estimator has a `predict_proba()` method, the decision function returns the class probabilities for each training instance.\n",
    "\n",
    "- **`bootstrap_features`**: as the name suggests, sample features. Though a bagging with trees that with both `bootstrap` and `bootstrap_features` enabled is not that different from random forest. \n",
    "\n",
    "- **`max_features`**: works similarly as `max_samples`, but works with features.\n",
    "\n",
    "There are other inputs that also allow for randomized features, presumably in the fashion of random forest.\n",
    "\n",
    "Note that parallerlization by `n_jobs`, while perhaps useful for bagging here or [random forest](random_forest.ipynb) since a lot of trees are built, might not benefit building a single, big tree.\n",
    "\n",
    "Also note that the `BaggingClassifier` automatically performs soft voting instead of hard voting if the base\n",
    "classifier can estimate class probabilities (i.e., if it has a `predict_proba()` method), which is the case\n",
    "with `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation, Metrics and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "- ESL, Section 8.7\n",
    "- [scikit-learn Document 1.11.1](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- < Hands-on Machine Learning >, Chapter 7.\n",
    "- MLEDU, Lecture 22.\n",
    "### Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
