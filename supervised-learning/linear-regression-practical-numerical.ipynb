{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552e6678",
   "metadata": {},
   "source": [
    "**Residual testing** to gauge whether linear regression is appropriate: [notebook](linear-regression-hypothesis-testing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92d1c96",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c2e06",
   "metadata": {},
   "source": [
    "### QR Decomposition - when the column number of $X$, $p$ is large\n",
    "\n",
    "When the column dimension of $X$, i.e. $p$, is extremely large, software usually utilizes QR Decomposition to calculate the OLS estimator.\n",
    "\n",
    "$$X = QR$$\n",
    "\n",
    "where $Q\\in \\mathbb{R}^{N\\times(p+1)}$ is an orthogonal matrix: $Q^{T}Q=I$, and $R\\in \\mathbb{R}^{(p+1)\\times(p+1)}$ is an upper triangular matrix. This decomposition is borned out of Gram-Schmidt procedure on the columns of $X$. As such, the OLS estimators can be expressed as\n",
    "\n",
    "$$\\hat{\\beta}=R^{-1}Q^{T}y$$\n",
    "$$\\hat{y}=QQ^{T}y$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd50718",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f718ef6",
   "metadata": {},
   "source": [
    "### There are too many data - when $N$ is large\n",
    "\n",
    "There may be situations where $N$ is so large, that $X$ cannot be fit into memory, let alone compute $X'X$. Another formula that we can rely on to compute $X'X$ is the following:\n",
    "\\begin{align}\n",
    "\\sum_{n=1}^Nx_n \\cdot x_n',\n",
    "\\end{align}\n",
    "where $x_n\\in R^{(p + 1) \\times 1}$ are the row vectors of $X$. That is, each $x_n \\cdot x_n'$ is a $(p + 1) \\times (p + 1)$ matrix. Assuming $p$ is not too large, we should be able to invert that matrix above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172545e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a47f2ac",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c88236d",
   "metadata": {},
   "source": [
    "### Imperfect Multi-Colinearity\n",
    "\n",
    "Like perfect multi-colinearity, imperfect multi-colinearity is a feature of the entire set of regressors. Unlike the perfect version, imperfect multi-colinearity does not prevent $(X'X)$ to be invertible, but depending on the numerical inversion precision, imperfect multi-colinearity can cause problems in practice. Theoretical-wise, **imperfect multi-colinearity causes the standard deviations of the OLS estimates to swell, and one can say that the OLS estimate is very inaccurate given the sample size. Thus it also calls into question the hypothesis-testing on regressors affected**.\n",
    "\n",
    "While it is somewhat easier to diagonize perfect multi-colinearity (e.g. did you fall for the dummy-variable trap, whereby you created a dummy variable for categorical regressors?), it can be hard to discern the reasons for imperfect multi-colinearity. The following are usually viewed as symptoms:\n",
    "- **Large changes in the estimated regression coefficients** when a predictor variable (column of $X$) is added or deleted, or when the data $y$ is perturbed.\n",
    "- **Insignificant regression coefficients for the affected variables** in the multiple regression, but a **rejection of the joint hypothesis that those coefficients are all zero** (using the F-test mentioned above).\n",
    "- If a **multivariable regression finds an insignificant coefficient** of a particular explanator, yet a **simple linear regression of the explained variable on this explanatory variable shows its coefficient to be significantly different from zero**, this situation indicates multicollinearity in the multivariable regression.\n",
    "- Estimate of **one regression slope is very largely positive, while another is very largely negative**.\n",
    "- **Correlation matrix** of the regressors can also be informative.\n",
    "- The **Variance Inflation Factor (VIF)** for a given regressor $j$: $VIF = \\frac{1}{1-R_j^2}$, where $R_j^2$ is the $R^2$ when regressing the $j$-th regressor on all other regressors. A **VIF of 5 or 10 and above indicates multi-colinearity**.\n",
    "- **Condition Number of $X$**. The condition number is the square root of the **maximum singular value of $X$ over the minimum singular value of $X$** (note that if we have a singular value to be zero, $X$ is not full-rank). If the condition number is **larger than 30**, it is ill-conditioned, and we have a multi-colinearity problem.\n",
    "\n",
    "\n",
    "Depending on the applications, especially when the it is not required to do hypothesis testing on $\\beta$, imperfect multi-colinearity need or need not to be remedied. But if it is needed to be dealt with, the following can be helpful.\n",
    "- Standardize your independent variables. This may help reduce a false flagging of a condition index above 30.\n",
    "- Drop one or some of the regressors if it or they are highly correlated with regressors to keep.\n",
    "- Resort to ridge, lasso regressions, or PLR or PLS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
