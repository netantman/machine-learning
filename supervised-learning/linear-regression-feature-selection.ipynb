{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a1d913c",
   "metadata": {},
   "source": [
    "### Regressors subset selection\n",
    "\n",
    "There are many reasons why one wants to simply select a subset of all regressors.\n",
    "\n",
    "- Adding unnecessary regressors does not help with the performance of the model, resulting in **higher standard errors for other regressors**, **introducing potential multi-colinearity**, and/or **reducing the adjust $R^2$**.\n",
    "- Too many regressors brings pressure in **efficiently interpreting the results**.\n",
    "- If subset selection is done via regularization methods, it can also be viewed to have the purpose of **improving the prediction accurary of the linear regression model**.\n",
    "\n",
    "The 'in-or-out' selection methods are described as follows, and regularizations will be covered next.\n",
    "\n",
    "- **Best-Subset Selection**\n",
    "\n",
    " > **cross-validation**: minimizing the prediction error \n",
    " \n",
    " > **AIC/BIC**: one chooses the smallest model that minimizes the KL-divergence vs the 'true model', or maximizes the Bayes factor; see the discussion on these two metrics in [this notebook](../meta-learning/evaluation-metrics-and-information-criterions.ipynb).\n",
    " \n",
    "- **Forward-stepwise Selection**: start with the intercept, and then greedily adds into the model the regressor the most improve the fit, usually by Z-score. \n",
    "    - There is a **clever updating algorithms by the QR decomposition** for the current fit to rapidly establish the next candidate. \n",
    "        - Suppose we have the QR decomposition for the $N\\times q$ matrix $X_1$ in a multiple regression problem with response\n",
    "$y$, and we have an additional $pâˆ’q$ predictors in the matrix $X_2$. \n",
    "        - Denote the current residual by $r$. The algorithm then works as follows. \n",
    "            - 1. Regress $X_2$ on the $Q$ matrix of $X_1$, obtain residuals. \n",
    "            - 2. Check which residuals decrease MSE the most when $r$ is regressed on it - choose that corresponding variable. \n",
    "            - 3. Update $Q$ and $r$.\n",
    "    - Another benefit of the forward-stepwise approach is that it can always be done, even when $p>>N$.\n",
    "\n",
    "- **Backward-stepwise Selection**: start with all the regressors, and then get rid of one regressor at a time for the least Z-score.\n",
    "\n",
    " > Backward selection can only be used when $N>p$, not as universally applicable as the forward approach. \n",
    " \n",
    " > Yet empirically the two have similar performance. \n",
    " \n",
    "- **Forward-Stagewise Selection**: introduced in Chapter 2 in [ESL](https://www.evernote.com/shard/s191/nl/21353936/c2a0e9ac-da49-4fee-8701-3cd70fc42134?title=The%20Elements%20of%20Statistical%20Learning), but is said to be useful particularly in high-dimensional problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
