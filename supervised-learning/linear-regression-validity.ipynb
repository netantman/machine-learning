{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0ac9cf",
   "metadata": {},
   "source": [
    "## Accessing the validity of linear regression\n",
    "\n",
    "Each of following, if present, results in **failure of the conditional mean zero least squares assumption**, i.e. $E(\\epsilon_n|x_n)\\neq 0$, which in turn means that the OLS estimator is biased and inconsistent. To be more precise, this is **not something that a large sample can help mitigate**.\n",
    "\n",
    "**Omitted variables**\n",
    " > Imagine the true model is given by $y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\epsilon$, where all the OLS assumptions are satisfied, $X$ is standardized and $Cov(X_1, X_2)=\\rho$. If instead we just regress $y$ on $X_1$, then the learned, limited regression model will not only try to capture the effect of $X_1$, but also the component of $X_2$ that is related to $X_1$. As such, the OLS estimate of the limited model will be converging w.p.1. to $\\beta_1+\\rho \\beta_2$, rather than $\\beta_1$. \n",
    " - When $\\rho\\neq 0$, this introduces biasedness.\n",
    " - In the special case of $\\rho= 0$, there is not biasedness, but he standard error is larger than should be.\n",
    "\n",
    "**Functional form misspecification**\n",
    " > Functional form misspecification arises when the functional form of the estimated regression function differs from the functional form of the population regression function (a typical case is when the relation is not linear). \n",
    " \n",
    " > If the functional form is misspecified, then the estimator of the partial effect of a change in one of the variables will, in general, be biased. \n",
    " \n",
    " > Functional form misspecification often can be detected by plotting the data and the estimated regression function, and it can be corrected by using a different functional form, such as the higher-order and/or cross-orders of regressors.\n",
    "\n",
    "**Errors in variables (measurement error in the regressors)**\n",
    "\n",
    " > Imagine there is one single regressor $X$, but it is observed as $\\tilde{X}=X+\\eta$.\n",
    " \n",
    " > If $corr(\\eta, X)=0$ (e.g. when there is measurement error not depending on $X$), then the OLS estimate of $y$ regressing on $\\tilde{X}$ converges to $\\beta\\times\\frac{\\sigma_X^2}{\\sigma_X^2+\\sigma_{\\eta}^2}$, biased. This is similar to the case of omitted variable problem when $\\rho\\neq 0$.\n",
    " \n",
    " > If $corr(\\eta, \\tilde{X})=0$ (e.g. when respondents respond to surveys using their best guess so $\\tilde{X}$ can be viewed as the conditional mean of $X$), then the OLS estimate of $y$ regressing on $\\tilde{X}$ is consistent, but the standard error is larger. This is similar to the case of omitted variable problem when $\\rho= 0$.\n",
    " \n",
    " > On the other hand, if the error is in $y$, then the OLS estimate is consistent, but standard error is larger - you can think of it as the error in $y$ simply lumps in the regression error.\n",
    "\n",
    "**Sample mis-selection and Missing Data**\n",
    "\n",
    " > **Missing at random**: the effect is to reduce sample size but does not introduce bias.\n",
    " \n",
    " > **Missing based on $X$**: the effect is to reduce sample size but does not introduce bias.\n",
    " \n",
    " > **Missing based on $y$**: will introduce bias, which is called **sample selection bias**.\n",
    " \n",
    " > The intuition is best gained by thinking about the regression line-through a scatter plot, and whether missing points from the scatter will change the regression line **on average**\n",
    "\n",
    "  >> If points are missing at random, the line does ot change on average.\n",
    "\n",
    "  >> If we delete points only about the value of X, the trend of the points will not change. nor will the regression line\n",
    "\n",
    "  >> If we only preserve the points depending on Y, say 0<Y<100, then the trend will be destroyed and the regression line will be changed.\n",
    " \n",
    "\n",
    "**Simultaneous causality**\n",
    "\n",
    " > $Y=\\beta_0+\\beta_1X+\\epsilon$ and $X=\\gamma_0+\\gamma_1Y+\\eta$. Obvious this introduces error as $X$ is correlated with $\\epsilon$.\n",
    "\n",
    "Many of the above issues can be remedied by the **instrumental-variable regression**. The gist of it is to find an instrument variable $Z$ that is correlated (and hence can represent $X$) but is not correlated with the error term $\\epsilon$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
