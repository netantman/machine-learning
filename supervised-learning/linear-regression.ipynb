{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification\n",
    "\n",
    "$$y = X\\beta + \\epsilon$$ \n",
    "\n",
    "Here $y\\in N\\times1$ is a vector of independent variables, $X\\in N\\times (p+1)$ is the design matrix, usually including the intercept - thus there are $p$ regressors, and $\\beta\\in p\\times 1$ are the intercept or slope coefficients. $E(\\epsilon)=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key assumptions and the OLS estimator\n",
    "The usual Ordinary Least Square *assumptions* are\n",
    "\n",
    "- A1: **Conditional Independence or Exogeneity** $E(\\epsilon_n|x_n)=0$: the error term conditioning on the corresponding regressors (not on other regressors $x_m$ where $m\\neq n$) has a mean of zero. If $X$ is normalized, this also implies $X$ is not correlated with $\\epsilon$.\n",
    "- A2: **i.i.d** $(x_n, y_n), n=1,\\dots, N$ are i.i.d. **Note**: can be relaxed when talking about autocorrelatedness in time-series regression.\n",
    "- A3: **No outliers**: $X$ and $y$ have nonzero finite fourth moments, i.e. outliers are unlikely. This serves as a reminder that the OLS estimators can be very sensitive to outliers, and hence somehow justifies the need of winsorization - the flooring and ceiling of $X$ and $y$ values at their high- and low-percent quantiles.\n",
    "    - To diagnose outliers, one can refer to **Cook's distance**.\n",
    "- A4: **Full rank of $X$** - **no perfect multicolinearity**, otherwise $(X^{T}X)^{-1}$ cannot be found. The definition and implications of imperfect colinearity will be discussed below.\n",
    "- A5: **Homogeneous Errors**: $var(\\epsilon_n|x_n)=\\sigma^2$. **Note**: later sections will talk about how to relax this and introduce heterogeneity\n",
    "- A6: **Conditional Normality**: $\\epsilon_n|x_n \\sim N(0, \\sigma^2)$ - this alone implies A1 and A5, though not necessarily needed homogeity.\n",
    "    - To ascertain A6, we can plot the residuals against normal in a **QQ plot**.\n",
    "\n",
    "The **Ordinary Least Square estimator** for $\\beta$ is $\\hat{\\beta}=(X^{T}X)^{-1}(X^{T}y)$. \n",
    "\n",
    "- If $X, y$ is viewed as the realizations of random vectors, then $(X^{T}X)$ is actually the sample variance of $X$, and $X^{T}y$ a sample covariance of $X$ and $y$, both up to a constant which is canceled in OLS above.\n",
    "\n",
    "With the OLS estimates, the **predicted values of $y$** is given by \n",
    "\n",
    "$$\\hat{y}=X\\hat{\\beta}=X(X^{T}X)^{-1}X^{T}y,$$\n",
    "\n",
    "where the matrix $H:=X(X^{T}X)^{-1}X^{T}$ is sometimes called the **hat matrix**, since it puts the hat on $y$. It is also a **projection matrix**, since $\\hat{y}$ is an orthogonal projection of $y$ on the linear subspace of $H$ - one consequence is $y-\\hat{y}$ is orthogonal to $\\hat{y}$. The estimate for $\\sigma^2$ is \n",
    "\n",
    "$$\\hat{\\sigma^2}=\\frac{1}{N-p-1}\\sum_{n=1}^N(y_n-\\hat{y_n})^2=\\frac{1}{N-p-1}y^{\\top}(I-H)y=\\frac{1}{N-p-1}SSR$$\n",
    "\n",
    "See description of SSR below. For the reasons that H is a projection matrix, $\\hat{\\sigma^2}$ is independent of $\\hat{\\beta}$.\n",
    "\n",
    "#### Intuitions about OLS estimator\n",
    "\n",
    "OLS $\\hat\\beta$ is solving for projection of $y$ onto the linear space of $X$, such that the errors are minimized.\n",
    "- In this way, the errors are the projection of y on the **orthogonal space of $X$**.\n",
    "- It is possible that while $y$ is almost orthogonal to either $x_1$ and $x_2$, but $x_1$ and $x_2$ together span $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Hypothesis Testing](linear-regression-hypothesis-testing)\n",
    "\n",
    "- Z-score and t-stat\n",
    "    - How does they change if data is duplicated, missing or otherwise changed?\n",
    "\n",
    "- F test\n",
    "    - What does it test about?\n",
    "    - What assumption of A1-A6 does it rely on?\n",
    "    - What is its relationship with the z-score above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Regressors subset selection](linear-regression-feature-selection.ipynb)\n",
    "\n",
    "- What are the motivations to do feature selection?\n",
    "\n",
    "- Best-Subset Selection\n",
    "    - cross-validation\n",
    "    - AIC/BIC\n",
    " \n",
    "- Forward-stepwise Selection\n",
    "    - There is a clever updating algorithms can exploit the QR decomposition.\n",
    "    - Can it be done when $p>>N$?\n",
    "\n",
    "- Backward-stepwise Selection\n",
    "    - Can it be done when $p>>N$?\n",
    " \n",
    "- Forward-Stagewise Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Shrinkage Methods](linear-regression-shrinkage.ipynb)\n",
    "\n",
    "- Are shrinkage scale invariant?\n",
    "- Do we penalize intercept in shrinkage? \n",
    "- Furthermore, what are exact mathematical formulation of L1 and L2 regularizations, and their equivalent constrained optimization?\n",
    "- What are ridge and lasso in a Bayesian perspective?\n",
    "- How does ridge and lasso behave for the principle components in $X$?\n",
    "- What is the motivation of elastic net? What is its mathematical formulation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Variants and Generalizations ](linear-regression-variants-generalizations.ipynb)\n",
    "\n",
    "- Time-series regression\n",
    "- HAC\n",
    "- LWLR\n",
    "- PCR\n",
    "- PLS\n",
    "\n",
    "Below are the usual regressions run in empirical asset pricing literature\n",
    "- Portfolio sort analysis\n",
    "- Fama-Macbeth\n",
    "- Two-Pass Cross Sectional Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Theoretical Properties](linear-regression-theoretical.ipynb)\n",
    "- Gauss Markov Theorem\n",
    "- Large- and Small-sample properties of the OLS estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "- Linear regression is very simple, and as such not likely to have a large variance or overfit the data.\n",
    "- The relation and impact of the regressors on the independent variables is very clear: just the components of $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "\n",
    "- Linear regression, assuming linear relations, can have a large bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Other Models\n",
    "\n",
    "- When one replaces the square loss with entropy or sigmoid loss, it becomes the [logistic regression](logistic_regression.ipynb).\n",
    "- High-order terms of the regressors and/or cross-moments can be easily incorporated as new regressors, and we venture into the land of non-linear regression, and related to [support vector machines](SVM.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages \n",
    "\n",
    "- Linear regressions are **very quick to implement and train**, compared to other machine learning models. And thus it is an **ideal base model**.\n",
    "- For prediction purposes, **linear regression models can sometimes outperform fancier nonlinear models, especially in situations with small number of training cases, low signal-to-noise ratio or sparse data**. This is related to the low-variance-high-biasedness feature of linear regressions. Note that this superior performance in forecasting need not have causal interpretation: if you see pedestrians carrying umbrellas, you might forecast rain, even though carrying an umbrella does not cause rain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "\n",
    "- For linear regressions, **additional data preprocessing may be required**. \n",
    "    - For example, it is known that outliers can greatly skew the estimates, so outliers need to be either winsorized (capped or floored by the corresponding quantiles) or the datapoint should be outright discarded - alas, in many applications this may not be desired, since outliers are also what actually happens in real life. Another way to rid outliers is via transforms, such as taking logs.\n",
    "- In the case where we have many regressors, one should also **be ware of perfect multi-colinearity** (in which the assumptions about OLS are violated) and imperfect multi-colinearity (see discussions below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details and Practical Tricks\n",
    "\n",
    "The `scikit-learn` package does not seem to have as many functionalities for linear regression as `statsmodels`. Thus in the following we mainly discuss the usage of `statsmodel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/netantman/anaconda3/lib/python3.7/site-packages/statsmodels/datasets/utils.py:344: FutureWarning: load will return datasets containing pandas DataFrames and Series in the Future.  To suppress this message, specify as_pandas=False\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   59.90</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 20 Jan 2022</td> <th>  Prob (F-statistic):</th> <td>3.02e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:19:37</td>     <th>  Log-Likelihood:    </th> <td> -52.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    21</td>      <th>  AIC:               </th> <td>   112.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    17</td>      <th>  BIC:               </th> <td>   116.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  -39.9197</td> <td>   11.896</td> <td>   -3.356</td> <td> 0.004</td> <td>  -65.018</td> <td>  -14.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.7156</td> <td>    0.135</td> <td>    5.307</td> <td> 0.000</td> <td>    0.431</td> <td>    1.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    1.2953</td> <td>    0.368</td> <td>    3.520</td> <td> 0.003</td> <td>    0.519</td> <td>    2.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.1521</td> <td>    0.156</td> <td>   -0.973</td> <td> 0.344</td> <td>   -0.482</td> <td>    0.178</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.713</td> <th>  Durbin-Watson:     </th> <td>   1.485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.700</td> <th>  Jarque-Bera (JB):  </th> <td>   0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.193</td> <th>  Prob(JB):          </th> <td>   0.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.107</td> <th>  Cond. No.          </th> <td>1.81e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.81e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.914\n",
       "Model:                            OLS   Adj. R-squared:                  0.898\n",
       "Method:                 Least Squares   F-statistic:                     59.90\n",
       "Date:                Thu, 20 Jan 2022   Prob (F-statistic):           3.02e-09\n",
       "Time:                        22:19:37   Log-Likelihood:                -52.288\n",
       "No. Observations:                  21   AIC:                             112.6\n",
       "Df Residuals:                      17   BIC:                             116.8\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        -39.9197     11.896     -3.356      0.004     -65.018     -14.821\n",
       "x1             0.7156      0.135      5.307      0.000       0.431       1.000\n",
       "x2             1.2953      0.368      3.520      0.003       0.519       2.072\n",
       "x3            -0.1521      0.156     -0.973      0.344      -0.482       0.178\n",
       "==============================================================================\n",
       "Omnibus:                        0.713   Durbin-Watson:                   1.485\n",
       "Prob(Omnibus):                  0.700   Jarque-Bera (JB):                0.140\n",
       "Skew:                          -0.193   Prob(JB):                        0.932\n",
       "Kurtosis:                       3.107   Cond. No.                     1.81e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.81e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "data = sm.datasets.stackloss.load()\n",
    "y = data.endog\n",
    "X = data.exog\n",
    "# Y = [1, 3, 4, 5, 2, 3, 4]\n",
    "# X = range(1,8)\n",
    "X = sm.add_constant(X) # handy for adding constants for regressors\n",
    "model = sm.OLS(y, X) # note that df and ssr calculation will be distorted by weights if we are using WLS\n",
    "results = model.fit() \n",
    "results.summary() # display all sorts of statistics and metrics: see the section on Results Interpretation, Metrics and Visualization below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels\n",
    "statsmodels.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178.82996159835858"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.ssr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178.82996159835858"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mse_resid * results.df_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.23463723, -1.91748529,  4.555533  ,  5.69777417, -1.71165358,\n",
       "       -3.0069397 , -2.38949071, -1.38949071, -3.1443789 ,  1.26719408,\n",
       "        2.63629676,  2.77946036, -1.42856088, -0.05049929,  2.36141836,\n",
       "        0.9050508 , -1.51995059, -0.45509295, -0.59825656,  1.41214728,\n",
       "       -7.23771286])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178.82996159835858"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.resid.dot(results.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-39.91967442,   0.7156402 ,   1.29528612,  -0.15212252])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needs to do a regularized fit, do `fit_regularized` instead: it can specify an elastic net whereby both lasso and ridge are special cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.base.elastic_net.RegularizedResultsWrapper at 0x7f7764fe6400>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_regularized(method='elastic_net', alpha=0.0, L1_wt=1.0, start_params=None, profile_scale=False, refit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some commonly used inputs\n",
    "- `method`: ‘elastic_net’ and ‘sqrt_lasso’ are currently implemented.\n",
    "- `alpha`: The penalty weight. If a scalar, the same penalty weight applies to all variables in the model. If a vector, it must have the same length as params, and contains a penalty weight for each coefficient.\n",
    "- `L1_wt`: The fraction of the penalty given to the L1 penalty term. Must be between 0 and 1 (inclusive). If 0, the fit is a ridge fit, if 1 it is a lasso fit.\n",
    "\n",
    "There are other inputs that allow for parallalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Further Practical Concerns](linear-regression-practical-numerical.ipynb)\n",
    "\n",
    "#### Hypothesis tests on the residuals of linear regression\n",
    "\n",
    "- Jarque-Bera test\n",
    "- Durbin-Watson test/Ljung-Box test\n",
    "- Omnibus test\n",
    "\n",
    "#### QR Decomposition: what is the formula of the OLS under QR decomposition of $X$?\n",
    "\n",
    "#### Is linear regression still possible when $N$ is too large to fit in the memory?\n",
    "\n",
    "#### Imperfect Colinearity: what are the diagostics and remedies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "- It is almost universal, even when linearity is not considered to be an appropriate assumption. It is used as a base case or benchmark.\n",
    "- Even if the regressors are not good in describing good *causal relations*, it can still be used for forecasting if $\\beta$ is stable and $R^2$ is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation, Metrics and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSS, ESS, SSR and Standard Errors\n",
    "\n",
    "The below set the foundation of measuring the goodness-of-fit for regression models.\n",
    "\n",
    "- **Total Sum of Squares**, or TSS, is the variation in the data of $y$: $TSS = \\sum_{n=1}^N(y_n-\\bar{y})^2$\n",
    "- **Explained Sum of Squares**, or ESS, is the variation explained by the estimated value of the linear regression: $ESS = \\sum_{n=1}^N(\\hat{y_n}-\\bar{y})^2$\n",
    "- **Sum of Squared Residuals**, or SSR, is portion that is unexplained by the regression: $SSR = \\sum_{n=1}^N(y_n-\\hat{y_n})^2$\n",
    "- **Standard Error**, or SER, is the unbiaed estimate of the standard deviation in the residuals, given the assumption it is homogenous: $SER = \\sqrt{\\frac{1}{N-p-1}SSR}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $R^2$ and adjusted $R^2$\n",
    "\n",
    "$R^2$ is the portion of variation in $y$ that is explained by the regression, so sometimes it is also called the **coefficient of determination**: \n",
    "\n",
    "$$R^2=\\frac{ESS}{TSS}=1-\\frac{SSR}{TSS}$$\n",
    "\n",
    "The adjusted $R^2$, or $\\bar{R^2}$, is a modified version of the $R^2$ that does not necessarily increase when a new regressor is added:\n",
    "\n",
    "$$\\bar{R^2}=1-\\frac{N-1}{N-p-1}\\frac{SSR}{TSS}$$.\n",
    "\n",
    "- Thus **$\\bar{R^2}$ is always less than $R^2$**. \n",
    "- Also, adding an additional regressor increases the ratio of sum of squares yet decreases the fraction of $\\frac{N-1}{N-p-1}$, and not necessarily increase $\\bar{R^2}$. \n",
    "- Finally, $\\bar{R^2}$ can be negative, when $SSR$ is not that smaller than $TSS$ to compensate for $p$ increasing by 1.\n",
    "\n",
    "$R^2$ and adjusted $R^2$ are the standard goodness-of-fit for linear regression models: adjusted $R^2$ can be used to compare different models. But there are things that $R^2$ will not tell you:\n",
    "\n",
    "- An increase in the $R^2$ and adjusted $R^2$ does **not necessarily mean that an added variable is statistically significant**. \n",
    "    - On the one hand, $R^2$ always increases when adding extra regressors. \n",
    "    - On the other hand, whether a new variable is statistically significant or not is the job of its Z-score, not $R^2$. In fact, one can have a high\n",
    "- A high $R^2$ and adjusted $R^2$ does **not mean that the regressors are a true cause of the dependent variable**, i.e. $R^2$ has nothing to do with causality.\n",
    "- A high $R^2$ and adjusted $R^2$ does **not mean that there is no omitted variable**, and thus a high $R^2$ does not indicate free of omitted variable bias.\n",
    "- A high $R^2$ and adjusted $R^2$ does not necessarily mean that you have the most appropriate set of regressors, nor does a low $R^2$ and adjusted $R^2$ necessarily mean that you have an inappropriate set of regressors - **it can be just that you have very low signal-to-noise ratio**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Accessing the validity of linear regression](linear-regression-validity.ipynb)\n",
    "\n",
    "- Omitted variables: the correlation between $X$ matters\n",
    "- Functional form misspecification\n",
    "- Error in variables: matters how the measurement error correlates with $X$\n",
    "- Sample mis-selection and missing data: different types\n",
    "- Simultaneous causality.\n",
    "\n",
    "**Incorrect calculation of the standard errors** also poses a threat to internal validity. Homoskedasticity-only standard errors are invalid if heteroskedasticity is present. If the variables are not independent across observations, as can arise in panel and time series data, then a further adjustment to the standard error formula is needed to obtain valid standard errors. See the discussion of HAC estimators above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "- [ESL](https://www.evernote.com/shard/s191/nl/21353936/c2a0e9ac-da49-4fee-8701-3cd70fc42134?title=The%20Elements%20of%20Statistical%20Learning_print12.pdf), Chapters 2, 18.6\n",
    "- [< Introduction to Econometrics, 3e >](https://www.evernote.com/shard/s191/nl/21353936/23a3b1a5-8f90-47a5-b796-d29931ba8db3?title=Introduction_to_Econometrics%EF%BC%8CUpdate%EF%BC%8C3e.pdf), Chapters 4-7, 9, 12, 14-16\n",
    "- [< Empirical Asset Pricing >](https://www.evernote.com/shard/s191/nl/21353936/fc98fb57-d94e-48fc-b709-610337ef92b0?title=G.BALI%EF%BC%8DEMPIRICAL_ASSET_PRICING_2016.pdf) 2016, Part I.\n",
    "- Wikipedia about colinearity\n",
    "- MLEDU, Lectures 6, 7.\n",
    "- [< Asset Pricing >](https://www.evernote.com/shard/s191/nl/21353936/b33b7ea8-e993-74b0-8d10-3c5e6795a578?title=Asset%20Pricing), Revised Edition. John Cochrane, 2005.\n",
    "- [Lars Lochstoer's lecture notes](https://www.evernote.com/shard/s191/nl/21353936/f0377478-137e-8dd9-5b05-29cf8da88dc4?title=Lochstoer's%20Asset%20Pricing%20II%20-%20Course%20Materials), Topic 1.\n",
    "\n",
    "### Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
