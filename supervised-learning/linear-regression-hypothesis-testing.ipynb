{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b84e6a2",
   "metadata": {},
   "source": [
    "### Hypothesis Testing on Coefficients\n",
    "\n",
    "- **Z-score**, or the t statistics for given regressor $\\beta_j$: $z_j = \\frac{\\hat{\\beta_j}}{\\hat{\\sigma}\\sqrt{v_j}}$, where $v_j$ is the $j$-th diagonal element of $(X^{T}X)^{-1}$ - the $z$-score can be easily constructed considering $\\beta$'s asymptotic normality, or that it has a normal distribution with A6 above.\n",
    "\n",
    " > The Z-score is to test the null hypothesis that $\\beta_j=0$.\n",
    " \n",
    " > Under the null hypothesis, $z_j$ is distributed as $t_{N-p-1}$. If $\\hat{\\sigma}$ is replaced with $\\sigma$, or if $N$ tends to infinity, then $z_j$ become a normal distribution. In practice, since the quantiles of Student's t and normal are similar, people use the normal quantiles oftentimes.\n",
    " \n",
    " > The above also informs the **confidence interval of the single regressor** $\\beta_j$.\n",
    "\n",
    "- **F test**, $F=\\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)}, (p_1>p_0)$ (see definitions of RSS below).\n",
    "\n",
    " > The situation is we have a bigger model with $p_1$ regressors, where the baseline model is with $p_0$ regressors. The null hypothesis is the extra $p_1-p_0$ regressors all have loadings of $0$. The F test is often used to 'justify' the linear regression, i.e. to test whether all regressor loadings are zero, or not.\n",
    "  \n",
    " > Under the null hypothesis, and especially A5 and A6, the F statistic will have a $F_{p_1-p_0, N-p_1-1}$ distribution. For large $N$, the quantiles of the $F_{p_1-p_0, N-p_1-1}$ distribution approaches that of $x^2_{p_1-p_0}/(p_1-p_0)$.\n",
    " \n",
    " > The above relies on A5. The **heterogeneity-robust F statistics has a different formula**; see Page 714 of [< Intro to Econometrics >](https://www.evernote.com/shard/s191/nl/21353936/23a3b1a5-8f90-47a5-b796-d29931ba8db3?title=Introduction_to_Econometrics%EF%BC%8CUpdate%EF%BC%8C3e.pdf). Many software may default to use the homogeneous version of the F statistics, though.\n",
    " \n",
    " > The above also informs the **confidence set of a set of regressors**.\n",
    "\n",
    "It can be shown that the Z-score $z_j$ above is equivalent to the F statistic for dropping the single coefficient $\\beta_j$ from the model, in that the **F stat is just the square of the Z-score**. The key to the proof of this lies in the Gram-Schmidt procedure, which finds the orthogonal vectors that span the sub-space of $X$. $\\beta_j/v_j$ above relates to the orthogonal vector $\\xi_j$ corresponding to $x_j$; see Page 54 of [ESL](https://www.evernote.com/shard/s191/nl/21353936/c2a0e9ac-da49-4fee-8701-3cd70fc42134?title=The%20Elements%20of%20Statistical%20Learning_print12.pdf) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25799c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e1e43e",
   "metadata": {},
   "source": [
    "### Hypothesis Testing on Residuals\n",
    "- **Jarque-Bera test**: testing of **skewness and kurtosis against Gaussian**; see discussion in this [notebook](../../other-quant-methods/hypothesis-testing.ipynb).\n",
    "- **Durbin-Watson test**: the Durbinâ€“Watson statistic is a test statistic used to **detect the presence of autocorrelation at lag 1 in the residuals (prediction errors) from a regression analysis**. This statistic is applied to the residuals from least squares regressions, and bounds are developed for the null hypothesis that the errors are serially uncorrelated against the alternative that they follow a first order autoregressive process. Note that the distribution of this test statistic does not depend on the estimated regression coefficients and the variance of the errors. A similar assessment can also be carried out for arbitrary lags in the [Ljung-Box test](../../other-quant-methods/hypothesis-testing.ipynb).\n",
    "- **Omnibus test**: Omnibus tests are a kind of statistical test. They test **whether the explained variance in a set of data is significantly greater than the unexplained variance, overall**. \n",
    "    - One example is the F-test in the analysis of variance. \n",
    "    - There **can be legitimate significant effects within a model even if the omnibus test is not significant**. \n",
    "        - For instance, in a model with two independent variables, if only one variable exerts a significant effect on the dependent variable and the other does not, then the omnibus test may be non-significant. \n",
    "        - This fact does not affect the conclusions that may be drawn from the one significant variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
