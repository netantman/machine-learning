{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a86604",
   "metadata": {},
   "source": [
    "### What is No Free-Lunch Theorem?\n",
    "\n",
    "In a famous 1996 [paper](https://en.wikipedia.org/wiki/No_free_lunch_theorem), David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the **No Free Lunch (NFL) theorem**. \n",
    "- For some datasets the best model is a linear model, while for other datasets it is a neural network. There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. \n",
    "- Since this is not possible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2e4ac",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638e3935",
   "metadata": {},
   "source": [
    "### What is the notion of the 'Unusually Effectiveness of Data'?\n",
    "\n",
    "In a famous paper published in 2001, Microsoft researchers Michele Banko and Eric Brill showed that very different Machine Learning algorithms, including fairly simple ones, performed almost identically well on a complex problem of natural language disambiguation once they were given enough data. \n",
    "\n",
    "As the authors put it, “these results suggest that we may want to reconsider the trade-off between spending time and money on algorithm development versus spending it on corpus development.”\n",
    "\n",
    "The idea that data matters more than algorithms for complex problems was further popularized by Peter Norvig et al. in a paper titled [The Unreasonable Effectiveness of Data](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf). \n",
    "- It should be noted, however, that small- and medium-sized datasets are still very common, and it is not always easy or cheap to get extra training data⁠—so don’t abandon algorithms just yet.\n",
    "\n",
    "This naturally leads to the notion of **model-centric vs data-centric model development**, which concerns whether you are keeping the data fixed and improve the model, or model fixed to improve on the data.\n",
    "\n",
    "Traditionally people focus on the quantity of model, but the quality of the data is important as well: improving the quality of data will allow multiple models to do well.\n",
    "- Try doing **data augmentation**, if synthetic yet realistic data can be added, especially on tags or categories where the algorithm is performing poorly.\n",
    "- Don't be afraid to **include or design features**. It might be the persception that some machine learning models, such as neural net, can automatically fulfill encoding and learn the features by themselves, but that may require a massive dataset, which is usually not the case in financial setting. In other words, it still pays to do feature engineering, driven by error analysis or expert opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c875c5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471e2f1",
   "metadata": {},
   "source": [
    "### Does adding more data hurt model performance?\n",
    "\n",
    "If\n",
    "- the model is large, i.e. low bias, e.g. think a neural net with many layers and wide widths)\n",
    "- the mapping distribution $p(y|x)$ is not noisy, e.g. the accuracy on the true OAS calculator is high with almost no numerical noise.\n",
    "\n",
    "Then adding data rarely hurts accuracy. In particular, data augmentation, though changes the distribution of $p(x)$ and potentially causes covariate drift, should be less of a concern here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
