{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ceb38f7",
   "metadata": {},
   "source": [
    "- One needs to be mindful of the relation between the number of samples vs. the number of predictors. Regularization may come in handy in these situations. Regularization is especially useful when $p>>N$, to overcome overfitting. Also become less costly to store and process data.\n",
    "- **Dimension reduction**\n",
    "     - Note: it is not always the case the more features the better \n",
    "     - Simple reason to the above is that there is the curse of dimensionality, as discussed in Chapter 2.5 in [< Elements of Statistical Learning>](https://www.evernote.com/shard/s191/nl/21353936/c2a0e9ac-da49-4fee-8701-3cd70fc42134?title=The%20Elements%20of%20Statistical%20Learning_print12.pdf)\n",
    "          - As dimension increases, the same amount of data points become sparser scattered in the feature space, and **neighborhood ceases to be local**. ESL discusses this as a problem for localization models such as kNN, but other machine learning methods suffer as well, since it is harder to find enough 'similar' instances to generalize from. Increasing the training examples may remedy this, the amount required is exponential in the number of dimensions.\n",
    "          - As dimension increases, **sample points are closer to the boundary of the feature space than other training data**, forcing the machine learning model to do more extrapolation rather than interpolation, as at inference time the target point is more likely to be outside of clusters of existing training data.\n",
    "          - As dimension increases, **numerical optimization takes longer** to reach global optima, and increases the likelihood of local optima.\n",
    "          - More dimensions also increases the likelihood of correlated features, which can cause a problem for models such as linear regression.\n",
    "- Regularization can also be considered a way to perform **feature selection**: helps generalization and model interpretability and visualization.\n",
    "     - Some regularization, such as lasso, will drive parameter values close to 0, so that numerical noise is eliminated or metigated.\n",
    "- Fits in the general theme that if **'noise' is introduced in the training phase, it may somehow generalize better**. You can also understand it in the perspective of 'shrinkage': by introducing a small bias, it is hoped that the variance will be greatly reduced so that the estimation error is reduced net-net."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
