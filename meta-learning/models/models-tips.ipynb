{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53f286a",
   "metadata": {},
   "source": [
    "For any given problem, if possible, **evaluate several models** during ML. \n",
    "- Before building a fancy ML model, invest some time building a **baseline model** \n",
    "     - For instance, a regularized linear model may help with understanding what/which predictors help.\n",
    "     - If the fancier model cannot beat linear, perhaps something is wrong with the fancy model (e.g. hyperparameter), or you simply don't have enough data to beat the simpler model.\n",
    "     - Needless to say, always prefer simpler model if performance is comparable, since the simpler one is usually cheaper to train and easier to deploy.\n",
    "     - By comparing the fancy model to the baseline model, sometimes it may help to shed light on what feature/model quality is key in the success of the fancy model. This is called **ablative analysis** in [Andrew Ng's slides](http://cs229.stanford.edu/materials/ML-advice.pdf)\n",
    "     \n",
    "- Model assessment for supervised machine learning problems are usually addressed using [**cross-validation**](cross-validation-and-backtesting.ipynb), which is an estimate of prediction error.\n",
    "- Traditional statistical learning, which emphasizes interpretability, often use [**information criterions and all kinds of metrics**](evaluation-metrics-and-information-criterions.ipynb), more than often calculated by resampling.\n",
    "- Visualization can be useful in gauging model performance (see the point above about scatterplot between ground-truth and fitted).\n",
    "- It may be commendable to take deployment constraints into account when picking the models.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6fe37",
   "metadata": {},
   "source": [
    "Get an **upper bound on achievable performance**, e.g. a model that achieves the Bayes classifier in classification, or human-level performance, or state-of-the-art or old system.\n",
    "  - This is to get an idea about the best performance you will get, and useful in scoping.\n",
    "  - For a given hypothesis space, or a class of model, one can **try overfit as much as possible without any regularization, possibly with a randomized sub-sample or even one sample in the training set**. If overfitting cannot be achieved, you probably need to think about a different model.\n",
    "  - Note that in comparing to human-level performance, **make sure human has the same data as the machine learning model**. For instance, it is not 'human will be able to detect the defect when looking at the phone', rather it is 'human can detect the defect by looking at the same pictures inspected by the machine learning algorithm'.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03a370",
   "metadata": {},
   "source": [
    "Avoid **premature statistical optimization**\n",
    "- Similar principle in software engineering.\n",
    "- Very often, it is not clear what parts of a system are easy or difficult to build, and which parts you need to spend lots of time focusing on.\n",
    "- The only way to find out **what needs work is to implement something quickly, and find out what parts break**; this echoes the best practice to collect just enough data to get started quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
