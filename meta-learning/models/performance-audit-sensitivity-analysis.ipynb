{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77893bcf",
   "metadata": {},
   "source": [
    "Though it seems overlaps with what is being done during ML, but consider this as a final tally of issues and/or have the conversation with stakeholders whether this is appropriate for production.\n",
    "\n",
    "Perform **performance auditing**: check for accuracy, fairness/bias, and other problems\n",
    "\n",
    "- Brainstorm the ways the system might go wrong\n",
    "    - Known or previously unchecked performance degrade on subsets of data (e.g. ethnicity, gender)\n",
    "    - How common are certain errors (e.g. False positive, false negative) and how it may break the system\n",
    "    - Performance on rare instances\n",
    "- Consider establishing metrics to assess performance against these issue on **appropriate slices of data** - at the very least be aware of potential risks. **Model remediation** are the techniques to address unequal model performance across different slices of models. Generally, there are three primary types of technical interventions for addressing bias concerns\n",
    "    - **Changing the input data**: Collecting more data, generating synthetic data, adjusting the weights and sampling rates of different slices, etc.\n",
    "    - **Intervening on the model**: Changing the model itself by introducing or altering model objectivies, adding constraints, etc. [MinDiff] is one such model remediation technique that seeks to balance error rates across different slices of the data by adding in the objective function the penalization of the difference of distributions of predictions across slices.\n",
    "    - **Post-processing the results**: Modifying the outputs of the model or the interpretation of the outputs to improve performance across metrics.\n",
    "\n",
    "Perform **sensitivity analysis**: how sensitive the trained model is to the features. \n",
    "   - What-if analysis\n",
    "   - Random inputs: in particular, sensitivity analysis should not be done using training data.\n",
    "   - Partial dependence plot; see [interpretability](interpretability-explanability.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d97f54",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Machine Learning Modeling Pipelines in Production: sensitivity analysis and adversarial attacks](https://www.coursera.org/learn/machine-learning-modeling-pipelines-in-production/lecture/bfawO/sensitivity-analysis-and-adversarial-attacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
