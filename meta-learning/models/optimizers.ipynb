{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a3d362",
   "metadata": {},
   "source": [
    "## Smart Optimization Schemes\n",
    "\n",
    "- **Why do we need to do SGD and/or BGD as opposed to full sample gradient calculations?**\n",
    "\n",
    "    - When **data set is too large**, including the full sample in gradient computation (which usually is the same of gradient at each sample point) is time consuming.\n",
    "    - By introducing randomness or uncertainties, SGD and BGD **avoid the optimization stuck in local minima**.\n",
    "\n",
    "- **What are the methods that extend SGD?**\n",
    "     \n",
    "    - **SGD with Nesterov's Accelerated Momentum**. Maintain a running average of all past steps, then **update with the average instead of the current descent**. \n",
    "        - The intuition is in directions in which the convergence is smooth, the average will have a large value; and in directions in which the estimate swings, the positive and negative swings will cancel out in the average (see comment below about the condition number of the Hessian matrix). Thus this will **effectively mitigate the zigzag behavior around the target minimum**. \n",
    "        - *The difference between Nesterov's method vs. traditional memuntum is that is changes the order of operations*: Nesterov takes the derivative after taking a further step in the past direction, then correction. Nesterov shows that this accelarate the convergence: the intuition being that, in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than using the gradient at the original position. \n",
    "\n",
    "    \\begin{align}\n",
    "    \\Delta \\textbf{W}^{(k)} &= \\mu \\Delta\\textbf{ W}^{(k-1)} - \\eta \\nabla_W L(\\textbf{W}^{(k-1)}+\\mu\\Delta W^{(k-1)})\\\\\n",
    "    \\textbf{W}^{(k)} &= \\textbf{W}^{(k-1)} + \\Delta\\textbf{W}^{(k)}\\\\\n",
    "    \\end{align}\n",
    "     \n",
    "    - **RMSProp**: **scale the learning rate by an estimate of the mean-squared derivative**: \n",
    "    \n",
    "    \\begin{align}\n",
    "    E[\\partial^2_w D]_k &= \\gamma E[\\partial^2_w D]_{k-1} + (1-\\gamma)(\\partial^2_w D)_k\\\\\n",
    "    w_{k+1}&=w_k - \\frac{\\eta}{\\sqrt{E[\\partial^2_w D]_k+\\epsilon}}\\partial_w D\n",
    "    \\end{align}\n",
    "     \n",
    "     See typical parameters in the implementation below.\n",
    "     \n",
    "     A similar optimization scheme to RMSProp is the **AdaGrad**, where the estimation of Hessian is simply the running *sum* of past second-order derivative. As such, the learning rate often gets decays too fast, usually before the global minimum is reached, and one can see that the loss function stops decreasing pretty soon (see visualization below). Thus AdaGrad is only suitable when the loss function is highly quadratic, in particular not the case for deep neural network.\n",
    "     \n",
    "    - **Adam**, which stands for adaptive moment estimation, **combines the ideas of Momentum optimization and RMSProp**. \n",
    "        - See typical parameters in the implementation below, which are also the ones recommended by the paper. \n",
    "        - Usually **Adam is the go-to method**, but it is still possible that for the problem at hand, other learning rate methods are superior.\n",
    "     \n",
    " - **Why do we need to do these?** The problems these methods are trying to solve are (a) avoid stuck in a narrow local minima; (b) avoids zigzag steps due to high condition number of parameter covariance or Hessian matrix (i.e. we can easily take a too big a step in one direction and too small to others), while avoiding having to normalize for the Hessian matrix. \n",
    " \n",
    "**Note**: the methods are **typically only concern with enhancing the first-order derivative methods**. This is because in deep neural network, there are typically tens of thousands of parameters and it is expensive or slow to compute the Hessian, if not difficult to fit in memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
