{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Types of Debugging\n",
    "\n",
    "- **Coding Mistakes**: when your model does not do what you want it to do (python, pytorch, math or logic errors)\n",
    "- **Training Mistakes**: when your model does what you want it to do, but is not learning well.\n",
    "- **Testing/Decoding Mistakes**: when your model is learning well but outputs bad results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Errors\n",
    "\n",
    "**Symptoms**\n",
    "\n",
    "- Loss does not decrease at all - obviously you need to plot the loss against epoches to show this\n",
    "- Output are constants\n",
    "- Training stops mid-time for unclear reasons.\n",
    "- Time issue: when everything works but is too slow. In your epochs, use the time module to check the duration of all your subtasks (data loading, forward, backward,...), and find the aberrant one.\n",
    "\n",
    "**Typical Mistakes**\n",
    "\n",
    "- You forgot to put your model in eval mode during inference time and your model is now producing garbage.\n",
    "- You passed softmax outputs to a loss that expects raw logits. E.g. passing softmax outputs to `nn.CrossEntropyLoss()`.\n",
    "\n",
    "**Advice**\n",
    "\n",
    "- Print everything to look for the first moment the problem appears. Be methodical.\n",
    "- Check your data: Not iterating ? Instance-label misalignment ? (spend a good amount of time checking if your data is sane. This is crucial especially if you’re doing some preprocessing on your data before passing it to your network.)\n",
    "- Check your shapes: everything consistent? (What does this mean?)\n",
    "- Check your hyperparameters: when you print them, are they what they are supposed to be?\n",
    "- Remember to turn off regularization - don't let them overwhelm your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Errors\n",
    "\n",
    "**Symptoms**\n",
    "\n",
    "For those errors, usually your loss does decrease, but not enough. Or when the training errors decrease but testing/validation errors do not. Or your model learns, training and validation loss decrease, but accuracy is low.\n",
    "\n",
    " > If you see absurdly low performance (or even random), it is probably a coding error. A good rule of thumb to check whether it is a training or coding error is that a random classification model would have a cross-entropy loss of approximately $\\log(\\text{number of classes})$.\n",
    "\n",
    "**Typical Mistakes**\n",
    "\n",
    "- Modelization issues : your model is too small to have sufficient capacity to learn patterns (or not well designed when the problem is complex)\n",
    "- Optimization issues : you cannot train your model properly\n",
    "- Overfitting : your model is too big/you train too long\n",
    "\n",
    "**Advice**\n",
    "\n",
    "- Try a few datapoints or a random subset of the training data, and try to see if your model can overfit it, i.e. training accuracy goes to 100% and validation accuracy much less. This can assure and inform if your model has enough capacity, and also beneficial in debugging coding errors too. Note that you need to first disable regularizations and other limiting features. Once you are certain your training loss decreases to 0, you can slowly add back regularizations and increase the data size (for which you should see loss increasing) and start fine tuning.\n",
    "- Check the below for optimization issues:\n",
    " > Learning rate: if too small, you will learn too slowly. If too large, you will learn for a while then diverge. Default “good” : 0.001. It is recommended to do learning rate decay : start large, then decrease (for\n",
    "example when loss stops improving).\n",
    "\n",
    " > Optimizer: (default “good” : Adam)\n",
    " \n",
    " > Initialization: (default “good” : xavier)\n",
    "\n",
    " > Batching (just the batch size on simple problems). Default “good” : from 32 to 256 if you can afford it.\n",
    "\n",
    " > Too deep models can create optimization problems too (vanishing gradients). They can also lead to overfitting.\n",
    " \n",
    " > Look at the activation/grad distribution per layer - recall the motivation of BatchNorm: it is not good to see any strange distribution.\n",
    "\n",
    "- Use the following tricks when you have an overfitting problem:\n",
    "\n",
    " > Verify that you shuffle your training data\n",
    " \n",
    " > Decrease your model size/depth\n",
    " \n",
    " > Use some of the tricks you know that help generalization : dropout, batchnorm, early stopping, validation-driven learning rate decay. Note : adaptative optimizers (Adam,...) overfit more.\n",
    " \n",
    " > It is also possible to overfit on the validation set. This happens when you try a very large amount of architectures/hyperparameters with the same validation set : you may find one that works “by chance” and won’t generalize. If you plan to look for many architectures, consider a better validation method like K-fold.\n",
    " \n",
    "- In classification problem, keep an eye on the accuracy, since the loss function is usually an approximation to the (step and indifferentiable) accuracy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple example\n",
    "\n",
    "We rely on the following MLP model as an example for what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([10000     2], shape=(2,), dtype=int32) tf.Tensor([10000], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Generating the data\n",
    "import math\n",
    "import numpy as np\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0-preview is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def sample_points(n):\n",
    "    \"\"\"\n",
    "    :param n: Total number of data-points\n",
    "    :return: A tuple (X,y) where X is a float tensor with shape (n,2)\n",
    "               and y is an interger tensor with shape(n,)\n",
    "    \"\"\"    \n",
    "    radius = np.random.random_sample(n) * 2\n",
    "    angle = np.random.random_sample(n) * 2 * math.pi\n",
    "    x = np.array([[r*np.cos(a), r*np.sin(a)] for (r, a) in zip(radius, angle)])\n",
    "    y = radius < 1\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = tf.convert_to_tensor(y)\n",
    "    return x, y\n",
    "\n",
    "X_train, y_train = sample_points(10000)\n",
    "X_valid, y_valid = sample_points(2000)\n",
    "X_val,y_val = sample_points(500)\n",
    "\n",
    "print(tf.shape(X_train), tf.shape(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                36        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 49\n",
      "Trainable params: 49\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build a simple MLP\n",
    "def build_model(dims, activation):\n",
    "    model = keras.models.Sequential()\n",
    "    for i in range(len(dims)):\n",
    "        if i == 0:\n",
    "            model.add(keras.layers.Dense(dims[0], input_shape=(2,), activation=activation)) # Note that you need to specify the input_shape, or otherwise the model will not build!\n",
    "        elif i < len(dims)-1:\n",
    "            model.add(keras.layers.Dense(dims[i], activation=activation))\n",
    "        else: # output layer\n",
    "            model.add(keras.layers.Dense(dims[i], activation=\"softmax\"))\n",
    "    return model\n",
    "\n",
    "# Test the function\n",
    "dims = [12, 1]\n",
    "model = build_model(dims, \"relu\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `kera.callbacks` for Printout Debug \n",
    "\n",
    "This can be done via custom callbacks. As an example of how to do that, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g., to detect overfitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, you can implement `on_train_begin()`, `on_train_end()`, `on_epoch_begin()`, `on_epoch_end()`, `on_batch_begin()`, and `on_batch_end()`. Callbacks can also be used during evaluation and predictions, should you ever need them (e.g., for debugging). For evaluation, you should implement `on_test_begin()`, `on_test_end()`, `on_test_batch_begin()`, or `on_test_batch_end()` (called by `evaluate()`), and for prediction you should implement `on_predict_begin()`, `on_predict_end()`, `on_predict_batch_begin()`, or `on_predict_batch_end()` (called by `predict()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bTQ-ExnoiAeY"
   },
   "source": [
    "## TensorBoard for Visualization\n",
    "\n",
    "TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, visualize the computation graph, analyze training statistics, view images generated by your model, visualize complex multidimensional data projected down to 3D and automatically clustered for you, and more! This tool is installed automatically when you install `tensorflow`.\n",
    "\n",
    "To use it, you must modify your program so that it outputs the data you want to visualize to special binary log files called *event files*. Each binary data record is called a *summary*. The TensorBoard server will monitor the log directory, and it will automatically pick up the changes and update the visualizations: this allows you to visualize live data (with a short delay), such as the learning curves during training. In general, you want to point the TensorBoard server to a root log directory and configure your program so that it writes to a different subdirectory every time it runs. This way, the same TensorBoard server instance will allow you to visualize and compare data from multiple runs of your program, without getting everything mixed up.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"tensorboard_logs\")\n",
    "def get_run_logdir():    \n",
    "    import time    \n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\") # You may want to add more info to the dir name to differentiate run info, such values of the hyperparameters, etc.   \n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras` provides a nice `TensorBoard()` callback, which takes the function you defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "train_val_ratio_cb = PrintValTrainRatioCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compile and fit the model. Notice how the `callbacks` defined in previous examples are speficied in `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      " 8032/10000 [=======================>......] - ETA: 0s - loss: 7.5349 - accuracy: 0.5086\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 2/100\n",
      " 8032/10000 [=======================>......] - ETA: 0s - loss: 7.5139 - accuracy: 0.5100\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 3/100\n",
      " 8032/10000 [=======================>......] - ETA: 0s - loss: 7.5712 - accuracy: 0.5062\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 4/100\n",
      " 8128/10000 [=======================>......] - ETA: 0s - loss: 7.4799 - accuracy: 0.5122\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 5/100\n",
      " 9952/10000 [============================>.] - ETA: 0s - loss: 7.5464 - accuracy: 0.5078\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 6/100\n",
      " 8512/10000 [========================>.....] - ETA: 0s - loss: 7.5675 - accuracy: 0.5065\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 7/100\n",
      " 8768/10000 [=========================>....] - ETA: 0s - loss: 7.5687 - accuracy: 0.5064\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 8/100\n",
      " 8960/10000 [=========================>....] - ETA: 0s - loss: 7.5776 - accuracy: 0.5058\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 9/100\n",
      " 8448/10000 [========================>.....] - ETA: 0s - loss: 7.5777 - accuracy: 0.5058\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 10/100\n",
      " 8512/10000 [========================>.....] - ETA: 0s - loss: 7.6072 - accuracy: 0.5039\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 11/100\n",
      " 8672/10000 [=========================>....] - ETA: 0s - loss: 7.5269 - accuracy: 0.5091\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 12/100\n",
      " 8960/10000 [=========================>....] - ETA: 0s - loss: 7.5725 - accuracy: 0.5061\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 13/100\n",
      " 8608/10000 [========================>.....] - ETA: 0s - loss: 7.5669 - accuracy: 0.5065\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 14/100\n",
      " 8800/10000 [=========================>....] - ETA: 0s - loss: 7.5325 - accuracy: 0.5088\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 15/100\n",
      " 9216/10000 [==========================>...] - ETA: 0s - loss: 7.5701 - accuracy: 0.5063\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 16/100\n",
      " 9728/10000 [============================>.] - ETA: 0s - loss: 7.5437 - accuracy: 0.5080\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 17/100\n",
      " 8768/10000 [=========================>....] - ETA: 0s - loss: 7.5442 - accuracy: 0.5080\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 18/100\n",
      " 8544/10000 [========================>.....] - ETA: 0s - loss: 7.5787 - accuracy: 0.5057\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 19/100\n",
      " 9952/10000 [============================>.] - ETA: 0s - loss: 7.5464 - accuracy: 0.5078\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 20/100\n",
      " 8992/10000 [=========================>....] - ETA: 0s - loss: 7.5302 - accuracy: 0.5089\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 21/100\n",
      " 9440/10000 [===========================>..] - ETA: 0s - loss: 7.5643 - accuracy: 0.5067\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 22/100\n",
      " 9472/10000 [===========================>..] - ETA: 0s - loss: 7.5776 - accuracy: 0.5058\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 23/100\n",
      " 9984/10000 [============================>.] - ETA: 0s - loss: 7.5545 - accuracy: 0.5073\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 24/100\n",
      " 9568/10000 [===========================>..] - ETA: 0s - loss: 7.5432 - accuracy: 0.5080\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 25/100\n",
      " 8352/10000 [========================>.....] - ETA: 0s - loss: 7.5932 - accuracy: 0.5048\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 26/100\n",
      " 8640/10000 [========================>.....] - ETA: 0s - loss: 7.5761 - accuracy: 0.5059\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 27/100\n",
      " 8704/10000 [=========================>....] - ETA: 0s - loss: 7.5275 - accuracy: 0.5091\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 28/100\n",
      " 8640/10000 [========================>.....] - ETA: 0s - loss: 7.5832 - accuracy: 0.5054\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 29/100\n",
      " 8160/10000 [=======================>......] - ETA: 0s - loss: 7.5219 - accuracy: 0.5094\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 30/100\n",
      " 7872/10000 [======================>.......] - ETA: 0s - loss: 7.5829 - accuracy: 0.5055\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 31/100\n",
      " 8544/10000 [========================>.....] - ETA: 0s - loss: 7.5248 - accuracy: 0.5092\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 32/100\n",
      " 8448/10000 [========================>.....] - ETA: 0s - loss: 7.5668 - accuracy: 0.5065\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      " 8320/10000 [=======================>......] - ETA: 0s - loss: 7.6150 - accuracy: 0.5034\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 34/100\n",
      " 8576/10000 [========================>.....] - ETA: 0s - loss: 7.6094 - accuracy: 0.5037\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 35/100\n",
      " 8640/10000 [========================>.....] - ETA: 0s - loss: 7.5406 - accuracy: 0.5082\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 36/100\n",
      " 8384/10000 [========================>.....] - ETA: 0s - loss: 7.5203 - accuracy: 0.5095\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 37/100\n",
      " 8832/10000 [=========================>....] - ETA: 0s - loss: 7.5798 - accuracy: 0.5057\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 38/100\n",
      " 8736/10000 [=========================>....] - ETA: 0s - loss: 7.5350 - accuracy: 0.5086\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 39/100\n",
      " 8448/10000 [========================>.....] - ETA: 0s - loss: 7.5595 - accuracy: 0.5070\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 40/100\n",
      " 8672/10000 [=========================>....] - ETA: 0s - loss: 7.5358 - accuracy: 0.5085\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 41/100\n",
      " 9408/10000 [===========================>..] - ETA: 0s - loss: 7.5428 - accuracy: 0.5081\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 42/100\n",
      " 9504/10000 [===========================>..] - ETA: 0s - loss: 7.5295 - accuracy: 0.5089\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 43/100\n",
      " 9312/10000 [==========================>...] - ETA: 0s - loss: 7.5398 - accuracy: 0.5083\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 44/100\n",
      " 9056/10000 [==========================>...] - ETA: 0s - loss: 7.5650 - accuracy: 0.5066\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 45/100\n",
      " 9472/10000 [===========================>..] - ETA: 0s - loss: 7.5533 - accuracy: 0.5074\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 46/100\n",
      " 9568/10000 [===========================>..] - ETA: 0s - loss: 7.5576 - accuracy: 0.5071\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 47/100\n",
      " 9920/10000 [============================>.] - ETA: 0s - loss: 7.5615 - accuracy: 0.5069\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 48/100\n",
      " 8768/10000 [=========================>....] - ETA: 0s - loss: 7.5442 - accuracy: 0.5080\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 49/100\n",
      " 8960/10000 [=========================>....] - ETA: 0s - loss: 7.5331 - accuracy: 0.5087\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 50/100\n",
      " 9120/10000 [==========================>...] - ETA: 0s - loss: 7.5523 - accuracy: 0.5075\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 51/100\n",
      " 8672/10000 [=========================>....] - ETA: 0s - loss: 7.5464 - accuracy: 0.5078\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 52/100\n",
      " 9376/10000 [===========================>..] - ETA: 0s - loss: 7.5178 - accuracy: 0.5097\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 53/100\n",
      " 9504/10000 [===========================>..] - ETA: 0s - loss: 7.5585 - accuracy: 0.5070\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 54/100\n",
      " 7744/10000 [======================>.......] - ETA: 0s - loss: 7.5577 - accuracy: 0.5071\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 55/100\n",
      " 8928/10000 [=========================>....] - ETA: 0s - loss: 7.5464 - accuracy: 0.5078\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 56/100\n",
      " 9760/10000 [============================>.] - ETA: 0s - loss: 7.5535 - accuracy: 0.5074\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 57/100\n",
      " 7872/10000 [======================>.......] - ETA: 0s - loss: 7.5634 - accuracy: 0.5067\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 58/100\n",
      " 8640/10000 [========================>.....] - ETA: 0s - loss: 7.5850 - accuracy: 0.5053\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 59/100\n",
      " 9280/10000 [==========================>...] - ETA: 0s - loss: 7.5030 - accuracy: 0.5107\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 60/100\n",
      " 8224/10000 [=======================>......] - ETA: 0s - loss: 7.5287 - accuracy: 0.5090\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 61/100\n",
      " 8800/10000 [=========================>....] - ETA: 0s - loss: 7.5377 - accuracy: 0.5084\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 62/100\n",
      " 9376/10000 [===========================>..] - ETA: 0s - loss: 7.5603 - accuracy: 0.5069\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 32us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 63/100\n",
      " 8128/10000 [=======================>......] - ETA: 0s - loss: 7.5912 - accuracy: 0.5049\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 64/100\n",
      " 8000/10000 [=======================>......] - ETA: 0s - loss: 7.5535 - accuracy: 0.5074\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7904/10000 [======================>.......] - ETA: 0s - loss: 7.5502 - accuracy: 0.5076\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 66/100\n",
      " 9824/10000 [============================>.] - ETA: 0s - loss: 7.5386 - accuracy: 0.5083\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 67/100\n",
      " 9568/10000 [===========================>..] - ETA: 0s - loss: 7.5256 - accuracy: 0.5092\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 68/100\n",
      " 9600/10000 [===========================>..] - ETA: 0s - loss: 7.5772 - accuracy: 0.5058\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 1s 55us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 69/100\n",
      " 8416/10000 [========================>.....] - ETA: 0s - loss: 7.5500 - accuracy: 0.5076\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 70/100\n",
      " 9888/10000 [============================>.] - ETA: 0s - loss: 7.5488 - accuracy: 0.5077\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 71/100\n",
      " 8928/10000 [=========================>....] - ETA: 0s - loss: 7.5911 - accuracy: 0.5049\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 72/100\n",
      " 8608/10000 [========================>.....] - ETA: 0s - loss: 7.5366 - accuracy: 0.5085\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 73/100\n",
      " 7936/10000 [======================>.......] - ETA: 0s - loss: 7.5681 - accuracy: 0.5064\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 74/100\n",
      " 8736/10000 [=========================>....] - ETA: 0s - loss: 7.5332 - accuracy: 0.5087\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 75/100\n",
      " 9216/10000 [==========================>...] - ETA: 0s - loss: 7.5518 - accuracy: 0.5075\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 76/100\n",
      " 8416/10000 [========================>.....] - ETA: 0s - loss: 7.5682 - accuracy: 0.5064\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 77/100\n",
      " 8576/10000 [========================>.....] - ETA: 0s - loss: 7.5200 - accuracy: 0.5096\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 78/100\n",
      " 9216/10000 [==========================>...] - ETA: 0s - loss: 7.5651 - accuracy: 0.5066\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 79/100\n",
      " 9216/10000 [==========================>...] - ETA: 0s - loss: 7.5185 - accuracy: 0.5097\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 80/100\n",
      " 8832/10000 [=========================>....] - ETA: 0s - loss: 7.5173 - accuracy: 0.5097\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 81/100\n",
      " 8640/10000 [========================>.....] - ETA: 0s - loss: 7.5459 - accuracy: 0.5079\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 82/100\n",
      " 8448/10000 [========================>.....] - ETA: 0s - loss: 7.5505 - accuracy: 0.5076\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 83/100\n",
      " 8832/10000 [=========================>....] - ETA: 0s - loss: 7.5607 - accuracy: 0.5069\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 84/100\n",
      " 8576/10000 [========================>.....] - ETA: 0s - loss: 7.5415 - accuracy: 0.5082\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 28us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 85/100\n",
      " 8512/10000 [========================>.....] - ETA: 0s - loss: 7.5549 - accuracy: 0.5073\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 30us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 86/100\n",
      " 9856/10000 [============================>.] - ETA: 0s - loss: 7.5515 - accuracy: 0.5075\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 87/100\n",
      " 8704/10000 [=========================>....] - ETA: 0s - loss: 7.5662 - accuracy: 0.5065\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 29us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 88/100\n",
      " 9056/10000 [==========================>...] - ETA: 0s - loss: 7.5633 - accuracy: 0.5067\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 89/100\n",
      " 9568/10000 [===========================>..] - ETA: 0s - loss: 7.5512 - accuracy: 0.5075\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 90/100\n",
      " 9248/10000 [==========================>...] - ETA: 0s - loss: 7.5506 - accuracy: 0.5076\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 27us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 91/100\n",
      " 9344/10000 [===========================>..] - ETA: 0s - loss: 7.5386 - accuracy: 0.5083\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 26us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 92/100\n",
      " 8864/10000 [=========================>....] - ETA: 0s - loss: 7.5784 - accuracy: 0.5058\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 93/100\n",
      " 9568/10000 [===========================>..] - ETA: 0s - loss: 7.5592 - accuracy: 0.5070\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 94/100\n",
      " 8608/10000 [========================>.....] - ETA: 0s - loss: 7.5401 - accuracy: 0.5082\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 95/100\n",
      " 9760/10000 [============================>.] - ETA: 0s - loss: 7.5519 - accuracy: 0.5075\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 96/100\n",
      " 9280/10000 [==========================>...] - ETA: 0s - loss: 7.5526 - accuracy: 0.5074\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9824/10000 [============================>.] - ETA: 0s - loss: 7.5386 - accuracy: 0.5083\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 1s 50us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 98/100\n",
      " 9536/10000 [===========================>..] - ETA: 0s - loss: 7.5637 - accuracy: 0.5067\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 48us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 99/100\n",
      " 8960/10000 [=========================>....] - ETA: 0s - loss: 7.5503 - accuracy: 0.5076\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n",
      "Epoch 100/100\n",
      " 9504/10000 [===========================>..] - ETA: 0s - loss: 7.5456 - accuracy: 0.5079\n",
      "val/train: 1.05\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 7.5501 - accuracy: 0.5076 - val_loss: 7.9350 - val_accuracy: 0.4825\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=0.1),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb, train_val_ratio_cb]) # Note how the callbacks are specified here\n",
    "                                               # Look for the 'val/train' in the logs below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this code, the `TensorBoard()` callback will take care of creating the log directory for you (along with its parent directories if needed), and during training it will create event files and write summaries to them. There’s one directory per run, each containing one subdirectory for training logs and one for validation logs. Both contain event files, but the training logs also include profiling traces: this allows TensorBoard to show you exactly how much time the model spent on each part of your model, across all your devices, which is great for locating performance bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at how to launch the UI for `TensorBoard` in Jupyter notebook. The first line loads the TensorBoard extension, and the second line starts a TensorBoard server on port 6006 (unless it is already started) and connects to it. Once it is up, you can open a web browser and go to http://localhost:6006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 1474), started 0:44:57 ago. (Use '!kill 1474' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5a77602e0b39720a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5a77602e0b39720a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=root_logdir --port=6007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, TensorFlow offers a lower-level API in the tf.summary package. The following code creates a SummaryWriter using the create_file_writer() function, and it uses this writer as a context to log scalars, histograms, images, audio, and text, all of which can then be visualized using `TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "        data = (np.random.randn(100) + 2) * step / 100 # some random data\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images\n",
    "        tf.summary.image(\"my_images\", images * step / 1000, step=step)\n",
    "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- CMU Deep Learning Course Fall 2019.\n",
    "- <Hands-on Machine Learning with Scikit-Learn, Keras, and Tensorflow>, 2nd Edition. Chapters 10, 11."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataVisualization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
