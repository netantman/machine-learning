{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Types of Debugging\n",
    "\n",
    "- **Coding Mistakes**: when your model does not do what you want it to do (python, pytorch, math or logic errors)\n",
    "- **Training Mistakes**: when your model does what you want it to do, but is not learning well.\n",
    "- **Testing/Decoding Mistakes**: when your model is learning well but outputs bad results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Errors\n",
    "\n",
    "**Symptoms**\n",
    "\n",
    "- Loss does not decrease at all - obviously you need to plot the loss against epoches to show this\n",
    "- Output are constants\n",
    "- Training stops mid-time for unclear reasons.\n",
    "- Time issue: when everything works but is too slow. In your epochs, use the time module to check the duration of all your subtasks (data loading, forward, backward,...), and find the aberrant one.\n",
    "\n",
    "**Typical Mistakes**\n",
    "\n",
    "- You forgot to put your model in eval mode during inference time and your model is now producing garbage.\n",
    "- You passed softmax outputs to a loss that expects raw logits. E.g. passing softmax outputs to `nn.CrossEntropyLoss()`.\n",
    "\n",
    "**Advice**\n",
    "\n",
    "- Print everything to look for the first moment the problem appears. Be methodical.\n",
    "- Check your data: Not iterating ? Instance-label misalignment ? (spend a good amount of time checking if your data is sane. This is crucial especially if you’re doing some preprocessing on your data before passing it to your network.)\n",
    "- Check your shapes: everything consistent? (What does this mean?)\n",
    "- Check your hyperparameters: when you print them, are they what they are supposed to be?\n",
    "- Remember to turn off regularization - don't let them overwhelm your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Errors\n",
    "\n",
    "**Symptoms**\n",
    "\n",
    "For those errors, usually your loss does decrease, but not enough. Or when the training errors decrease but testing/validation errors do not. Or your model learns, training and validation loss decrease, but accuracy is low.\n",
    "\n",
    " > If you see absurdly low performance (or even random), it is probably a coding error. A good rule of thumb to check whether it is a training or coding error is that a random classification model would have a cross-entropy loss of approximately $\\log(\\text{number of classes})$.\n",
    "\n",
    "**Typical Mistakes**\n",
    "\n",
    "- Modelization issues : your model is too small to have sufficient capacity to learn patterns (or not well designed when the problem is complex)\n",
    "- Optimization issues : you cannot train your model properly\n",
    "- Overfitting : your model is too big/you train too long\n",
    "\n",
    "**Advice**\n",
    "\n",
    "- Try a few datapoints or a random subset of the training data, and try to see if your model can overfit it, i.e. training accuracy goes to 100% and validation accuracy much less. This can assure and inform if your model has enough capacity, and also beneficial in debugging coding errors too. Note that you need to first disable regularizations and other limiting features. Once you are certain your training loss decreases to 0, you can slowly add back regularizations and increase the data size (for which you should see loss increasing) and start fine tuning.\n",
    "- Check the below for optimization issues:\n",
    " > Learning rate: if too small, you will learn too slowly. If too large, you will learn for a while then diverge. Default “good” : 0.001. It is recommended to do learning rate decay : start large, then decrease (for\n",
    "example when loss stops improving).\n",
    "\n",
    " > Optimizer: (default “good” : Adam)\n",
    " \n",
    " > Initialization: (default “good” : xavier)\n",
    "\n",
    " > Batching (just the batch size on simple problems). Default “good” : from 32 to 256 if you can afford it.\n",
    "\n",
    " > Too deep models can create optimization problems too (vanishing gradients). They can also lead to overfitting.\n",
    " \n",
    " > Look at the activation/grad distribution per layer - recall the motivation of BatchNorm: it is not good to see any strange distribution.\n",
    "\n",
    "- Use the following tricks when you have an overfitting problem:\n",
    "\n",
    " > Verify that you shuffle your training data\n",
    " \n",
    " > Decrease your model size/depth\n",
    " \n",
    " > Use some of the tricks you know that help generalization : dropout, batchnorm, early stopping, validation-driven learning rate decay. Note : adaptative optimizers (Adam,...) overfit more.\n",
    " \n",
    " > It is also possible to overfit on the validation set. This happens when you try a very large amount of architectures/hyperparameters with the same validation set : you may find one that works “by chance” and won’t generalize. If you plan to look for many architectures, consider a better validation method like K-fold.\n",
    " \n",
    "- In classification problem, keep an eye on the accuracy, since the loss function is usually an approximation to the (step and indifferentiable) accuracy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bTQ-ExnoiAeY"
   },
   "source": [
    "## TensorBoard for Visualization\n",
    "\n",
    "TensorBoard is a neural network visualization library developed by Google as part of Tensorflow. In the past, people can use TensorBoard in PyTorch via third-party adaptors like tensorboardX. Starting from 1.2.0 (the latest version), **PyTorch officially supports TensorBoard**. We recommend you to use the latest version of PyTorch and use its built-in support of TensorBoard for visualization.\n",
    "\n",
    "This tutorial covers how to use PyTorch's official support of TensorBoard. You can also refer to the [official documentation](https://pytorch.org/docs/stable/tensorboard.html). If you insist on using an older version of PyTorch, try [tensorboardX](https://github.com/lanpa/tensorboardX).\n",
    "\n",
    "Let's take up the same task as defined in Recitation 2. We'll be training a Neural Network to classify if a set of points $(x_1, x_2)$ lie inside a circle of radius $1$ or not. For more details on what the task is, please re-visit Recitation 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UwK3leNfLxwe"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install torch>=1.2.0 tensorboard future tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vXkLZjiiAec"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WdKH8sD_iAek"
   },
   "source": [
    "Similar to Recitation 2, we first sample some polar co-ordinates that are randomly distributed within a circle of radius 2 and centered at origin, ie. $(0,0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIvpTW7giAel"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sample_points(n):\n",
    "    \"\"\"\n",
    "    :param n: Total number of data-points\n",
    "    :return: A tuple (X,y) where X is a float tensor with shape (n,2)\n",
    "               and y is an interger tensor with shape(n,)\n",
    "    \"\"\"    \n",
    "    radius = torch.rand(n) * 2\n",
    "    angle = torch.rand(n) * 2 * math.pi\n",
    "    x1 = radius * angle.cos()\n",
    "    x2 = radius * angle.sin()\n",
    "    y = radius < 1\n",
    "    x = torch.stack([x1, x2], dim=1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3229,
     "status": "ok",
     "timestamp": 1568841355900,
     "user": {
      "displayName": "Liwei Cai",
      "photoUrl": "",
      "userId": "16466533746106471957"
     },
     "user_tz": 240
    },
    "id": "1JXyeDBkiAeq",
    "outputId": "5cac8bca-59fa-4287-c7fd-4e9f4c5266b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 2]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# Generating the data\n",
    "\n",
    "X_train, y_train = sample_points(10000)\n",
    "X_val,y_val = sample_points(500)\n",
    "\n",
    "print(X_train.size(), y_train.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3227,
     "status": "ok",
     "timestamp": 1568841355900,
     "user": {
      "displayName": "Liwei Cai",
      "photoUrl": "",
      "userId": "16466533746106471957"
     },
     "user_tz": 240
    },
    "id": "IxjmcoMDrojK",
    "outputId": "59a6457e-d704-4dd1-dd2e-2dcc249479dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=12, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=12, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build a simple MLP\n",
    "def build_model(dims, activation):\n",
    "  layers = []\n",
    "  for i in range(len(dims) - 1):\n",
    "    layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "    if i < len(dims) - 2:\n",
    "      layers.append(activation())\n",
    "  return nn.Sequential(*layers) # Recall that if you don't do this, pytorch will not automatically register all params in the module.\n",
    "\n",
    "# Test the function\n",
    "print(build_model([2, 12, 1], nn.Sigmoid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moSbTVIziAeb"
   },
   "source": [
    "A SummaryWriter writes all values we want to visualize to a given directory. This line creates a SummaryWriter that creates write event files and saves in the `./runs/example` directory.\n",
    "```python\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"./runs/example\")\n",
    "```\n",
    "**You should use different run directories in a common root directory for different runs of your model.** TensorBoard looks for runs in the root directory. So for this example, we start the TensorBoard with:\n",
    "\n",
    "```sh\n",
    "tensorboard --logdir=./runs\n",
    "```\n",
    "Then, visit `localhost:6006` with your browser to see the TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7l7fn7viAe0"
   },
   "source": [
    "Each time we add a value, we specify a **tag** and a **step**. Each tag is a string and corresponds to a plot on TensorBoard. The step is an integer (`epoch` in this example) that serves as the X axis on the plot.\n",
    "\n",
    "To plot a single scalar, use [*SummaryWriter.add_scalar()*](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalar). To plot multiple scalars on a plot, use [*SummaryWriter.add_scalars()*](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalars) and pass in a dict of scalars.\n",
    "\n",
    "Using [*SummaryWriter.add_histogram()*](https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_histogram) to plot a histogram of values in a tensor is also useful for understanding the dynamics of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6cat4BvXxn3Z"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, writer, epochs=1000):\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters())\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    model.zero_grad()\n",
    "    out = model(X_train).flatten()\n",
    "    loss = criterion(out, y_train.float())\n",
    "    train_loss = loss.item()\n",
    "    train_acc = ((out > 0) == y_train).float().mean().item()\n",
    "    \n",
    "    loss.backward()\n",
    "    # Plot histogram of gradient of all parameters\n",
    "    for name, param in model.named_parameters():\n",
    "      writer.add_histogram('grad_' + name, param.grad.data, epoch)\n",
    "    optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      out = model(X_val).flatten()\n",
    "      val_loss = criterion(out, y_val.float()).item()\n",
    "      val_acc = ((out > 0) == y_val).float().mean().item()\n",
    "    # Plot loss and accuracy on train and val\n",
    "    writer.add_scalars('loss', {'train': train_loss, 'val': val_loss}, epoch)\n",
    "    writer.add_scalars('acc', {'train': train_acc, 'val': val_acc}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWJ8UNjFiAfA"
   },
   "source": [
    "## Using Different Activation functions\n",
    "\n",
    "Let's see and understand how each of these activation functions perform.\n",
    "\n",
    "- Sigmoid\n",
    "    * Get values between 0 and 1.\n",
    "    * A Sigmoid layer easily dies or saturates. A value too small kills the gradient flow whereas a value too big saturates the neurons, effectively passing no information through it.\n",
    "    \n",
    "- Tanh\n",
    "    * Outputs values between -1 and 1. Also zero centered and so does not have the problem of all positive/negative gradients.\n",
    "    * Better than Sigmoid but problem of saturation persists.\n",
    "\n",
    "- ReLU\n",
    "    * Converges quickly as is a threshold based activation and does not saturate.\n",
    "    * Neurons die off. Large weight update could set the weights in such a way (they become negative) during backpropagation that they never fire for any data point. Important to set lower learning rates for ReLU.\n",
    "    \n",
    "    \n",
    "**TRY IT OUT**\n",
    "\n",
    "Use all the 3 activation functions. See which performs better and try to find out why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 45823,
     "status": "ok",
     "timestamp": 1568841398500,
     "user": {
      "displayName": "Liwei Cai",
      "photoUrl": "",
      "userId": "16466533746106471957"
     },
     "user_tz": 240
    },
    "id": "YXJykUfUiAfU",
    "outputId": "92382484-1f24-4ab2-9dbb-55aab719eba2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 70.95it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 68.54it/s]\n",
      "100%|██████████| 1000/1000 [00:13<00:00, 72.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"./runs/sigmoid\")\n",
    "model = build_model([2, 12, 1], nn.Sigmoid)\n",
    "train(model, writer)\n",
    "\n",
    "writer = SummaryWriter(\"./runs/tanh\")\n",
    "model = build_model([2, 12, 1], nn.Tanh)\n",
    "train(model, writer)\n",
    "\n",
    "writer = SummaryWriter(\"./runs/relu\")\n",
    "model = build_model([2, 12, 1], nn.ReLU)\n",
    "train(model, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TZqK0YxbNoZm"
   },
   "source": [
    "Open TensorBoard and see the result! (These graphs can be found in Recitation 5 of )\n",
    "\n",
    "![TensorBoard: accuracy](tensorboard_acc.png)\n",
    "\n",
    "![TensorBoard: gradient distribution](tensorboard_grad.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DataVisualization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
