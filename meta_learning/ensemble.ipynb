{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble learning can be seen as a post-learning practice that benefits the so-called *wisdom of the crowd*, aggregating predictions of a group of predictors (such as classifiers or regressors) to usually get better predictions than\n",
    "with the best individual predictor. The group of predictors is called an **ensemble**; thus, this technique is called **Ensemble Learning**, and an Ensemble Learning algorithm is called an Ensemble method. Generally, the net result is\n",
    "that the ensemble has a similar bias but a lower variance than a single predictor trained on the\n",
    "original training set\n",
    "\n",
    "[Boosting](../supervised_learning/boosting.ipynb), [bagging](../supervised_learning/bagging.ipynb) and [random forests](../supervised_learning/random_forest.ipynb) are formal examples of ensemble learning that relies on [trees](../supervised_learning/CART.ipynb). Here in this notebook, we focus on ensembles itself.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we implement Voting Classifiers in `sklearn`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want a **hard-voting**, or **majority-vote** classifier, ensembling a random forest, logistic regression and SVC. In `sklearn` we proceed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "voting='hard')\n",
    "#voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do **soft-voting**, i.e. the voted class is the one having the highest average class probability, all ensembled classifiers have to have `predict_proba()`. This is not the case, e.g. for the `SVC` class by default, so you need to set its `probability` hyperparameter to `True` (recall that this will make the `SVC` class use cross-validation to estimate class probabilities, slowing down training, and it will add a `predict_proba()` method). Then in `VotingClassifier`, set `voting` to `'soft'`, and you are done. \n",
    "\n",
    "Usually, soft-voting will achieve higher performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "- < Hands-on Machine Learning >, Chapter 7."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
