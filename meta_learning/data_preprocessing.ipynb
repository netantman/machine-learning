{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering** is how the predictors are encoded.\n",
    "\n",
    "   - Some encoding may be optimal for some models but poor for others.\n",
    "   - In some models, multiple encodings of the same data may cause problems, except for some feature selection algos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Transformation on Single Predictors**\n",
    "\n",
    " - Centering and Scaling: may lost the interpretability of data\n",
    " - Resolve skewness/heavy-tailness: make the data symmetric by log square root inverse, or the more general Box-Cox (1964) Transform\n",
    " \\begin{align}\n",
    " x^{*}=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  \\frac{x^{\\lambda}-1}{\\lambda},\\;\\;\\text{if }\\lambda\\neq0\\\\\n",
    "                  \\log(x)\\;\\;\\text{if }\\lambda=0,\\\\\n",
    "                \\end{array}\n",
    "              \\right.\n",
    " \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers Removel**\n",
    "\n",
    " - Outliers can be spotted via visualization.\n",
    " - Should take caution to remove outliers, especially when sample size is small, for fear of losing important distributional info - do investigate these outliers.\n",
    " - Some models are robust to outliers: Trees, SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Reduction and Feature Extraction: PCA** - see the notebook of [PCA](PCA.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing Values**\n",
    "\n",
    " - It is important to know why the data is missing - it can inform the analysis: missing at random vs. missing not at random.\n",
    " - Censored data: exact data is missing but we know something of its value - in predictive modeling, it is common to treat them as missing values or just us the censored values as observed.\n",
    " - Missing values are usually concentrated more according to predictors (cols of the data table) rather than data points (the rows of the data table). So removing predictors may be preferable.\n",
    " - On the implementation side, check out `fillna`, `dropna` functions in `pandas`, depending on what the simple way you want to pursue concerning missing data. `sklearn` also has a class called `SimpleImputer` that can systematically fill in missing values by mean, median, most frequent values, etc.\n",
    " - Some models are robust to missing values, such as Trees, MARS and kNN **(from 10.7 in ESL, need to think about this...)**\n",
    " - The most \"official\" way to tackle missing values is perhaps by imputation: explore the inter-relationship between predictors, using e.g. kNN. There is a small literature; see Further Reading.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Predictors**\n",
    "\n",
    " - **Zero-variance predictors**: low fraction of unique values with respect to entire population, or large ratio of the frequency of the most prevalent values to the second.\n",
    " - **Colinearity**: (1) Use Variance Inflation Factor in [linear models](linear_regression.ipynb) to diagnoze. (2) If a few of the top PCA components represent high precentage of variance, it indicates high colinearity; loadings of the factors also helps indicate such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Predictors**\n",
    "\n",
    " - Dummy variables\n",
    " - For classifications, class centroids can be calculated and each data point's distance can be added as an extra predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binning Predictors**\n",
    "\n",
    "**Manual** categorization is discouraged since it generally harm performance, although in some cases it might improve interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- Applied Predictive Modeling, Chapters 3\n",
    "- < Hands on Machine Learning >, Chapter 2\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Applied Predictive Modeling, Chapters 12-15\n",
    "- Saar-Tsechansky M, Provost F (2007b). \"Handling Missing Values When Applying Classification Models\" Journal of Machine Learning Research, 8, 1625-1657.\n",
    "- Jerez J, Molina I, Garcia-Laencina P, Alba R, Ribelles N, Martin M, Franco L (2010). \"Missing Data Imputation Using Statistical and Machine Learning Methods in a Real Breast Cancer Problem.\" Artificial Intelligence in Medicine, 50, 105-115."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
