{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Cross-Validations (CV) used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is first and foremost used to provide an **estimate and statistics of the prediction errors** when the model is release to the wild.\n",
    "\n",
    "An implication of that is the model can be used to **choose for hyperparameters** (e.g. those that determines the degree of regularitzation) or even **perform feature selection**. \n",
    "- For example, sequential forward selection (SFS) of features uses a nested holdout procedure to first pick the best individual feature, by looking at all models built using just one feature. After choosing a first feature, SFS tests all models that add a second feature to this first chosen feature. The best pair is then selected. Next the same procedure is done for three, then four, and so on. When adding a feature does not improve classification accuracy on the validation data, the SFS process stops. \n",
    "- There is a similar procedure called sequential backward elimination of features. As you might guess, it works by starting with all features and discarding features one at a time. It continues to discard features as long as there is no performance loss.\n",
    "\n",
    "Note that using cv to pick hyperparameters suffer from so-called **'multiple comparisons'** (e.g., choosing the best complexity for a model by comparing many complexities). , i.e. many multiple statistical tests are run and then simply the results that look good are picked. Thus cv can only be seen as a safeguard to model overfitting, rather than a guarantee of model generalization. This is perhaps also the reason why you would see test error usually higher than what CV has estimated it to be.\n",
    "\n",
    "Often a 'one-standard error' rule is used with cross-validation, in which we choose the most parsimonious model whose error is no more than one standard error above the error of the best model (though sometimes parsimonious may be ambiguous). The standard deviation can be estimated by the CV across the folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the different types of cross validations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation\n",
    "\n",
    "Let $\\kappa : \\{1, . . . , N\\} \\rightarrow \\{1, . . . , K\\}$ be an indexing function that indicates the partition to which observation $i$ is allocated by the randomization. Denote by $\\hat{f}^{-k}(x)$ the fitted function, computed with the $k$-th part of the data removed. Then the cross-validation estimate of prediction error is\n",
    "\\begin{align}\n",
    "CV(\\hat{f}) = \\frac{1}{N}\\sum_{n=1}^NL(y_n, \\hat{f}^{-\\kappa(n)}(x_n)),\n",
    "\\end{align}\n",
    "where $L$ denote the loss function. Note that CV only leaves out the data in fitting, while the prediction loss is taken over all the training samples. Put in another way, one needs to train $\\kappa$ models, and predict all-over the samples, though in this way, we are summing over in-sample and out-of-sample errors. This is probably why in the `sklearn` implementation, the validation error is computed out of the validation set, not the whole training set\n",
    "\n",
    "### Leave-One-Out\n",
    "\n",
    "The case where $K=N$ is known as **leave-one-out cross validation**. \n",
    "- With $K=N$, the cross-validation estimator is a relatively unbiased for the true (expected) prediction error, but can have high variance because the $N$ 'training sets' by leaving only one out are so similar to one another. The computational burden is also considerable, requiring to train $N$ models.\n",
    "- On the other hand, with K = 5 say, cross-validation has lower variance. But bias could be a problem, depending on how the performance of the learning method varies with the size of the training set. \n",
    "\n",
    "Overall, five- or tenfold cross-validation are recommended as good compromises. \n",
    "\n",
    "By the way, it is also possible to do 'Leave-P-Out' in `sklearn`.\n",
    "\n",
    "### Repeated K-fold\n",
    "\n",
    "Just repeat K-fold several times, each time with a different split arrangment by randomness.\n",
    "\n",
    "### Stratified K-fold\n",
    "\n",
    "Variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. This is recommended for classification problems with imbalanced classes.\n",
    "\n",
    "### Approximations: Generalized Cross-validation\n",
    "\n",
    "Generalized cross-validation provides a convenient approximation to leaveone out cross-validation, for linear fitting under squared-error loss.\n",
    "\n",
    "A linear fitting method is one for which we can write\n",
    "\\begin{align}\n",
    "\\hat{y} = Sy.\n",
    "\\end{align}\n",
    "It can be show that for many linear fitting methods, \n",
    "\\begin{align}\n",
    "\\frac{1}{N}\\sum_{n=1}^N[y_n-\\hat{f}^{-n}(x_n)]^2 = \\frac{1}{N}\\sum_{n=1}^N\\left[\\frac{y_n-\\hat{f}(x_n)}{1-S_{nn}}\\right]^2,\n",
    "\\end{align}\n",
    "where $S_{nn}$ is the $n$-th diagonal element of $S$. The GCV approximation is then \n",
    "\\begin{align}\n",
    "GCV{\\hat{f}} = \\frac{1}{N}\\sum_{n=1}^N\\left[\\frac{y_n-\\hat{f}(x_n)}{1-trace(S)/N}\\right]^2,\n",
    "\\end{align}\n",
    "by appealing to the approximation $1/(1-x)^2\\approx 1+2x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the usual blunder/wrong way in doing cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is called **data leakage**, whereby information is inadvently shared between the training and test set, or between the training and validation set within a fold, therefore making the test set or validation set not a good proxy to 'data that we will see if the model is released out in the wild'.\n",
    "\n",
    "Though this concept seems straightforward, data leakage can be sneaky. Following are by no means an exhaustive list of scenarios that trip people up.\n",
    "\n",
    "#### Inproper pre-processing\n",
    "\n",
    "- This is an example in Section 7.10.2 in ESL, where features are chosen on the *whole train set* before cv is carried out. Since which feature is important now depends on the whole set information, it sabotages the effectiveness of cv.\n",
    "\n",
    "- Another example is when one looks at why model performs badly on test data, and improve accordingly (see below). But in this sense, optimizing hyper-parameter using cv is suspicious; see the comment about 'multiple comparison' above.\n",
    "\n",
    "- In cv, the model should be completely re-trained using just the train data in that fold, though evaluation of errors is on the whole data set.\n",
    "\n",
    "- When trying to impute missing data, sample mean of the whole data set rather than the training set is used, for instance.\n",
    "\n",
    "- Accidentally call `fit_transform` instead of `transform` when applying the same `Pipeline` in `sklearn` to your test data before making prediction - you do not want to fit the test set!\n",
    "\n",
    "#### Duplicates\n",
    "\n",
    "When train and test sets have identical data (duplications).\n",
    "\n",
    "#### Temporal applications\n",
    "\n",
    "When future information is used to train the model and test on the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If a model, after cross-validated, performs badly in the test data. Can we go back and tune the model parameters/hyper-parameters further so that it performs better in the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one tries to look at how the model performs badly on some instances of the training data, seek to remedy those and that brings improvement of model performance on the test data, it is acceptable. \n",
    "\n",
    "On the other hand, a big taboo is to directly look at instances in the test data to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `sklearn` implementation details of cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "estimator = svm.SVC(kernel='linear', C=1)\n",
    "scores = cross_val_score(estimator, X, y, cv=5, scoring=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs of `cross_val_score`\n",
    "- **`estimator`**: estimator object with method `fit`, the object to use to fit the data\n",
    "- **`X`**: data to fit\n",
    "- **`y`**: labels\n",
    "- **`scoring`**: A `str` (see [evaluation_metrics_and_information_criterions](evaluation_metrics_and_information_criterions.ipynb)) or a scorer callable object/function with signature `scorer(estimator, X, y)` which should return only a single value.\n",
    "- **`cv`**: `int`, cross-validation generator or an iterable, default=None\n",
    "Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "    \n",
    "    - `None`, to use the default 5-fold cross validation for stratified KFold,\n",
    "\n",
    "    - `int`, to specify the number of folds in a `(Stratified)KFold`,\n",
    "\n",
    "    - CV splitter (see below),\n",
    "\n",
    "    - An iterable yielding (train, test) splits as arrays of indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are CV splitters in `sklearn`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV splitters are also called cross validation iterators in `sklearn` [document](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators). It is a generator, which implments `split` method on `X` that produces indices that can be used to generate dataset splits according to different cross validation strategies. It is also be passed as an argument to `cv` in `cross_val_score`. The usual ones mentioned above all have `sklearn` implmentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=2)\n",
    "# Repeated K-fold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n",
    "# Leave One Out\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "# ShuffleSplit - not mentioned above, but simply a user defined number of independent train/test splits.\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
    "# Stratified K-fold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use `sklearn` and cv to fine-tune hyperparameters of the models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exhaustive Grid Search\n",
    "\n",
    "`GridSearchCV` uses cross-validation to evaluate all the possible combinations of hyperparameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "estimator = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(estimator, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search.fit(X, y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All inputs are those with `cross_val_score`. One of the few things worths mentioning is `param_grid`. It can either be a `dict` of a list of `dict`, where the keys are the hyperparameter to the `estimator`, and values are lists of candidate values of that hyperparameter. For each element (`dict`) in the list of `param_grid`, all combinations of hyperparameters in the list are tried in the cv.\n",
    "\n",
    "Another input to `GridSearchCV` is `refit`, which is `True` by default: once it finds the best estimator using cross-validation, it retrains it on the whole training set. This is usually a good idea, since feeding it more data will likely improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Parameter Optimization\n",
    "\n",
    "The grid search approach is fine when you are exploring relatively few combinations, but when the hyperparameter search space is large, the ususal curve of dimensionality arises. Just as Monte Carlo methods is the answer to high-dimension difficulty in numerical integration, it is often preferable to use `RandomizedSearchCV` instead. This class can be used in much the same way as the `GridSearchCV` class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.\n",
    "\n",
    "Specifying how parameters should be sampled is done using a dictionary, very similar to specifying parameters for `GridSearchCV`. Additionally, a computation budget, being the number of sampled candidates or sampling iterations, is specified using the `n_iter` parameter. For each parameter, either a distribution over possible values or a list of discrete choices (which will be sampled uniformly) can be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a good visualization tool for fine-tuning models using CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proper way of choosing multiple hyperparameters of an estimator are of course grid search or similar methods that select the hyperparameter with the maximum score on a validation set or multiple validation sets. Note that if we optimized the hyperparameters based on a validation score the validation score is biased and not a good estimate of the generalization any longer. To get a proper estimate of the generalization we have to compute the score on another test set.\n",
    "\n",
    "However, it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values.\n",
    "\n",
    "The function `validation_curve` can help in this case, which computes training and test scores for varying parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y = load_iris(return_X_y=True)\n",
    "indices = np.arange(y.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X, y = X[indices], y[indices]\n",
    "estimator=Ridge()\n",
    "\n",
    "train_scores, valid_scores = validation_curve(estimator, X, y, \"alpha\",\n",
    "...                                               np.logspace(-7, 3, 3),\n",
    "...                                               cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs of `validation_curve` is similar to that of `cross_val_score` aforementioned, except for the following\n",
    "\n",
    "- **`param_name`** `str` Name of the parameter that will be varied.\n",
    "\n",
    "- **`param_range`** array-like of shape `(n_values,)`.  The values of the parameter that will be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between a learning curve and a validation curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning curve shows the generalization performance—the performance only on testing data, plotted against the amount of training data used. A validation graph shows the generalization performance as well as the performance on the training data, but plotted against some model parameter. Validation graphs generally are shown for a fixed amount of training data.\n",
    "\n",
    "The implementation of `learning_curve` is introduced in [debugging-ml-algorithms](debugging-ml-algorithms.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a good CV strategy for time-series?"
   ]
  },
  {
   "attachments": {
    "time-series-split.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a rolling split, to make sure we do not use the future to predict the past:\n",
    "\n",
    "![time-series-split.PNG](attachment:time-series-split.PNG)\n",
    "\n",
    "It is implemented in the cv splitter `timeseriessplit` in `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- < Data Science for Business >, Chapter 5.\n",
    "- ESL, Chapter 7.10\n",
    "- sklearn document, [Section 3.1](https://scikit-learn.org/stable/modules/cross_validation.html), [3.2](https://scikit-learn.org/stable/modules/grid_search.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
