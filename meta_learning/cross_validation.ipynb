{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Cross-Validations (CV) used for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is first and foremost used to provide an **estimate and statistics of the prediction errors** when the model is release to the wild.\n",
    "\n",
    "An implication of that is the model can be used to **choose for hyperparameters** (e.g. those that determines the degree of regularitzation) or even **perform feature selection**. \n",
    "- For example, sequential forward selection (SFS) of features uses a nested holdout procedure to first pick the best individual feature, by looking at all models built using just one feature. After choosing a first feature, SFS tests all models that add a second feature to this first chosen feature. The best pair is then selected. Next the same procedure is done for three, then four, and so on. When adding a feature does not improve classification accuracy on the validation data, the SFS process stops. \n",
    "- There is a similar procedure called sequential backward elimination of features. As you might guess, it works by starting with all features and discarding features one at a time. It continues to discard features as long as there is no performance loss.\n",
    "\n",
    "Note that using cv to pick hyperparameters suffer from so-called 'multiple comparisons' (e.g., choosing the best complexity for a model by comparing many complexities). , i.e. many multiple statistical tests are run and then simply the results that look good are picked. Thus cv can only be seen as a safeguard to model overfitting, rather than a guarantee of model generalization.\n",
    "\n",
    "Often a 'one-standard error' rule is used with cross-validation, in which we choose the most parsimonious model whose error is no more than one standard error above the error of the best model (though sometimes parsimonious may be ambiguous). The standard deviation can be estimated by the CV across the folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the different types of cross validations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation\n",
    "\n",
    "Let $\\kappa : \\{1, . . . , N\\} \\rightarrow \\{1, . . . , K\\}$ be an indexing function that indicates the partition to which observation $i$ is allocated by the randomization. Denote by $\\hat{f}^{-k}(x)$ the fitted function, computed with the $k$-th part of the data removed. Then the cross-validation estimate of prediction error is\n",
    "\\begin{align}\n",
    "CV(\\hat{f}) = \\frac{1}{N}\\sum_{n=1}^NL(y_n, \\hat{f}^{-\\kappa(n)}(x_n)),\n",
    "\\end{align}\n",
    "where $L$ denote the loss function. Note that CV only leaves out the data in fitting, while the prediction loss is taken over all the training samples. Put in another way, one needs to train $\\kappa$ models, and predict all-over the samples, though in this way, we are summing over in-sample and out-of-sample errors.\n",
    "\n",
    "### Leave-One-Out\n",
    "\n",
    "The case where $K=N$ is known as **leave-one-out cross validation**. \n",
    "- With $K=N$, the cross-validation estimator is a relatively unbiased for the true (expected) prediction error, but can have high variance because the $N$ 'training sets' by leaving only one out are so similar to one another. The computational burden is also considerable, requiring to train $N$ models.\n",
    "- On the other hand, with K = 5 say, cross-validation has lower variance. But bias could be a problem, depending on how the performance of the learning method varies with the size of the training set. \n",
    "\n",
    "Overall, five- or tenfold cross-validation are recommended as good compromises.\n",
    "\n",
    "### Approximations: Generalized Cross-validation\n",
    "\n",
    "Generalized cross-validation provides a convenient approximation to leaveone out cross-validation, for linear fitting under squared-error loss.\n",
    "\n",
    "A linear fitting method is one for which we can write\n",
    "\\begin{align}\n",
    "\\hat{y} = Sy.\n",
    "\\end{align}\n",
    "It can be show that for many linear fitting methods, \n",
    "\\begin{align}\n",
    "\\frac{1}{N}\\sum_{n=1}^N[y_n-\\hat{f}^{-n}(x_n)]^2 = \\frac{1}{N}\\sum_{n=1}^N\\left[\\frac{y_n-\\hat{f}(x_n)}{1-S_{nn}}\\right]^2,\n",
    "\\end{align}\n",
    "where $S_{nn}$ is the $n$-th diagonal element of $S$. The GCV approximation is then \n",
    "\\begin{align}\n",
    "GCV{\\hat{f}} = \\frac{1}{N}\\sum_{n=1}^N\\left[\\frac{y_n-\\hat{f}(x_n)}{1-trace(S)/N}\\right]^2,\n",
    "\\end{align}\n",
    "by appealing to the approximation $1/(1-x)^2\\approx 1+2x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the usual blunder/wrong way in doing cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is called **data leakage**, whereby information is inadvently shared between the training and test set, or between the training and validation set within a fold, therefore making the test set or validation set not a good proxy to 'data that we will see if the model is released out in the wild'.\n",
    "\n",
    "Though this concept seems straightforward, data leakage can be sneaky. Following are by no means an exhaustive list of scenarios that trip people up.\n",
    "\n",
    "#### Inproper pre-processing\n",
    "\n",
    "- This is an example in Section 7.10.2 in ESL, where features are chosen on the *whole train set* before cv is carried out. Since which feature is important now depends on the whole set information, it sabotages the effectiveness of cv.\n",
    "\n",
    "- Another example is when one looks at why model performs badly on test data, and improve accordingly (see below). But in this sense, optimizing hyper-parameter using cv is suspicious; see the comment about 'multiple comparison' above.\n",
    "\n",
    "- In cv, the model should be completely re-trained using just the train data in that fold, though evaluation of errors is on the whole data set.\n",
    "\n",
    "#### Duplicates\n",
    "\n",
    "When train and test sets have identical data (duplications).\n",
    "\n",
    "#### Temporal applications\n",
    "\n",
    "When future information is used to train the model and test on the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If a model, after cross-validated, performs badly in the test data. Can we go back and tune the model parameters/hyper-parameters further so that it performs better in the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one tries to look at how the model performs badly on some instances of the training data, seek to remedy those and that brings improvement of model performance on the test data, it is acceptable. \n",
    "\n",
    "On the other hand, a big taboo is to directly look at instances in the test data to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a good CV strategy for time-series?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Look at MLEDU Slides 1, Page 41, among others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference between a learning curve and a validation curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning curve shows the generalization performanceâ€”the performance only on testing data, plotted against the amount of training data used. A validation graph shows the generalization performance as well as the performance on the training data, but plotted against some model parameter. Validation graphs generally are shown for a fixed amount of training data.\n",
    "\n",
    "The implementation of `learning_curve` is introduced in [debugging-ml-algorithms](debugging-ml-algorithms.ipynb). In what follows we discuss `validation_curve` in `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- < Data Science for Business >, Chapter 5.\n",
    "- ESL, Chapter 7.10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
