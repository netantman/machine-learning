{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advice and Good Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive modeling is a term that encompasses machine learning, artificial intelligence, etc., that focuses on **predictions**. While the primary interest of predictive modeling is to generate accurate predictions, a second concern is **interpretability**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the importance of **expert judgments**:\n",
    "   - \"In the end, predictive modeling is not a substitute for intuition, but a compliment.\"\n",
    "   - \"Traditional experts make better decisions when they are provided the results of statistical predictions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to watch out in preparation of **data**\n",
    "- **Leakage**: Information about labels sneaks into features\n",
    "- **Sample bias**: Test inputs and deployment inputs have different distributions\n",
    "- **Nonstationary**: When the thing you are modeling changes over time\n",
    "    - **Covaraiate Shift**: input distribution changes over time\n",
    "    - **Concept Shift**: correct output for given input changes over time\n",
    "    - **Domain Shift**: the discrepancy between training and inference data.\n",
    "\n",
    "Look at [data preprocessing](data_preprocessing.ipynb) for more detailed suggestions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to **understand the predictors** (also see the point next)\n",
    " - Predictor sets may contain **numerically redundant information**.\n",
    " - Predictor sets may contain **missing values**.\n",
    " - Predictors may be **sparse**\n",
    " - Predictors may not be relevant to responses - **feature selection** is the process of determining the minimum set of relevant predictors needed.\n",
    " - One needs to be mindful of the relation between the number of samples vs. the number of predictors. Regularization may come in handy in these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally a good practice to **visualize the data**: see `scatterplot` from `pandas.plotting`, and there is also a `scatter` option to choose from in `DataFrame.plot`.\n",
    "\n",
    "**Before ML**\n",
    "  - If the predictors are of low-dimension, a scatterplot probably suffices. \n",
    "  - If there are multiple predictors, plots that help understand the cross-relationship between predictors are needed\n",
    "  \n",
    "**After ML**\n",
    "  - Check both in-sample and out-of-sample model fit vs. ground-truth, again you can use the scatter plot. Better yet to identify cases where model does a lousy job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any given problem, if possible, **evaluate several models**.\n",
    " - Before building a fancy ML model, invest some time building a linear (baseline) model\n",
    "     - Build a regularized linear model may help with understanding what/which predictors help.\n",
    "     - If the fancier model cannot beat linear, perhaps something is wrong with the fancy model (e.g. hyperparameter), or you simply don't have enough data to beat the simpler model.\n",
    "     - Needless to say, always prefer simpler model if performance is comparable, since the simpler one is usually cheaper to train and easier to deploy.\n",
    "     - By comparing the fancy model to the baseline model, sometimes it may help to shed light on what feature/model quality is key in the success of the fancy model. This is called ablative analysis in [Andrew Ng's slides](http://cs229.stanford.edu/materials/ML-advice.pdf)\n",
    " - Often it is also helpful to get an upper bound on achievable performance, e.g. a model that achieves the Bayes classifier in classification. \n",
    "     - This is to get an idea about the best performance you will get.\n",
    "     - For a given hypothesis space, or a class of model, one can try overfit as much as possible without any regularization, possibly even with a randomized sub-sample in the training set. If overfitting cannot be achieved, you probably need to think about a different model.\n",
    " - Model assessment for supervised machine learning problems are usually addressed using [**cross-validation**](cross_validation.ipynb), which is an estimate of prediction error.\n",
    " - Traditional statistical learning, which emphasizes interpretability, often use [**information criterions and all kinds of metrics**](evaluation_metrics_and_information_criterions.ipynb), more than often calculated by resampling.\n",
    " - Visualization can be useful in gauging model performance (see the point above about scatterplot between ground-truth and fitted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avoid **premature statistical optimization**\n",
    "- Very often, it is not clear what parts of a system are easy or difficult to build, and which parts you need to spend lots of time focusing on.\n",
    "- The only way to find out what needs work is to implement something quickly, and find out what parts break.\n",
    "\n",
    "(Similar principle in software engineering.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- Applied Predictive Modeling, Chapters 1-2\n",
    "- MLEDU: Lectures 1, 12, 14.\n",
    "- < Hands-on Machine Learning >, Chapter 2\n",
    "- Andrew Ng's slides: [Advice for Applying Machine Learning](http://cs229.stanford.edu/materials/ML-advice.pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
