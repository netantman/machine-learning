{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two kinds of mathematical formulations for PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Manifold Approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have $N$ data points in the $p$-dimension space: $x_1, \\dots, x_N$. Imagine we have a linear manifold parametrized by $\\lambda$:\n",
    "\n",
    "$$f(\\lambda)=\\mu+V_q\\lambda,$$\n",
    "\n",
    "where $\\mu$ is a location vector in $\\mathbb{R}^p$, $V_q\\in\\mathbb{R}^{p\\times q}$ with $q$ orthogonal unit vectors as columns, and $\\lambda$ is a $q$-dimensional vector. This is an affine hyperplane of rank $q$. Suppose the task is to approximate the $N$ data points by this $q$-dimensional manifold by least square:\n",
    "\n",
    "$$\\min_{\\mu, \\{\\lambda_i\\}, V_q}\\sum_{i=1}^N\\left|x_i-\\mu-V_q\\lambda_i\\right|^2.$$\n",
    "\n",
    "The optimal values for $\\mu$ and $\\lambda_i$ are given by \n",
    "\n",
    "$$\\hat{\\mu}=\\bar{x}$$\n",
    "$$\\hat{\\lambda_i}=V_q^{\\top}(x_i-\\bar{x})$$\n",
    "\n",
    "and $V_q$ is given by the first $q$ columns of $V$ in the **Singular Value Decomposition (SVD)** of $X\\in\\mathbb{R}^{N\\times p}$:\n",
    "\n",
    "$$X=UDV^{\\top}.$$\n",
    "\n",
    "Here $U\\in\\mathbb{R}^{N\\times p}$ is an orthogonal matrix, $V\\in\\mathbb{R}^{p\\times p}$ also an orthogonal matrix and $D\\in\\mathbb{R}^{p\\times p}$ a positive definite diagonal matrix.\n",
    "\n",
    "- The diagonal elements of $D$ are known as **singular values**.\n",
    "- The columns of $UD$ are the **principle components**. The first $q$ elements the $N$ rows of $UD$ give $\\hat{\\lambda}_i, i=1,\\dots, N$. That is, dimension reduction from $p$ to $q$ is done by representing $x_i$ by $\\hat{\\lambda}_i$, and $q$ principle components are needed.\n",
    "- Columns of $V$ are called **principle directions** or **loadings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Linear Combination that has the largest variation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the data has been normalized, with each of the $p$ predictors having $0$ mean and unit variance. The first column $v_1$ of $V$ above is the optimal solution to the following optimization problem\n",
    "\n",
    "$$\\max_v v^{\\top}X^{\\top}Xv$$\n",
    "\n",
    "$$s.t. v^{\\top}v=1$$\n",
    "\n",
    "The second column $v_2$ of $V$ satistifies unit vector orthogonal to $v_1$ and has the largest variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants and Generalizations\n",
    "\n",
    "- Principal curves and surfaces generalize PCA, in that PCA projects data points to hyperplanes, while principle curves and surfaces project points to non-linear manifolds.\n",
    "\n",
    "- [Spectral clustering and kernel PCA](spectral_clustering_and_kernel_PCA.ipynb) also generalize PCA, in that features are blown up to higher dimension before PCA is applied. This [post](https://stats.stackexchange.com/questions/94463/what-are-the-advantages-of-kernel-pca-over-standard-pca) explains why this blown-up-to-high-dimension nonlinearity is needed. See also the discussion near Figure 14.29 in ESL.\n",
    "\n",
    "- We often interpret principle components by examining the direction vectors or loading, to see which variables play a role. Often this interpretation is made easier if the loadings are sparse. This motivates the sparse PCA technique. There are many different formulations for the sparse PCA problem. \n",
    "   - The one implemented in **`sklearn`** solves the following optimization problem.\n",
    "\n",
    "    $$(U^{\\ast}, V^{\\ast})=arg\\min_{U,V}\\frac{1}{2}||X-UV||_2^2+\\alpha||V||_1,$$\n",
    "\n",
    "        subject to $||U_k||_2=1$ for all $0\\leq k < n_{components}$. \n",
    "   - The one in ESL, taken from Zou, et al. (2006), resembles the elastic net, with both L1 and L2 penalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "- The variations in the first few principle components usually account for much of the total variations, especially in cases where there are significant correlations among the $p$ predictors, or equivalently, when $X$ has a high condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "\n",
    "- PCA is only linear. When the clustering or variation is nonlinear, PCA tends to fail.\n",
    "- PCA is sensitive in predictor scales. In fact, all predictors are recommended to be normalized before PCA is carried out; otherwise, there can be some contrived example where the high mean masks the true maximum variation in the data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details and Practical Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA in sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99244289  0.00755711]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PCA' object has no attribute 'singular_values_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec0b16c4cadc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msingular_values_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PCA' object has no attribute 'singular_values_'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "pca = PCA(n_components=2, whiten=False)\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)  \n",
    "#print(pca.singular_values_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selected Parameters**\n",
    "\n",
    "- **`n_components`**: \n",
    "\n",
    "Number of components to keep. If n_components is not set all components are kept: `n_components == min(n_samples, n_features)`\n",
    " - If `n_components == ‘mle’` and `svd_solver == ‘full’`, Minka’s MLE is used to guess the dimension \n",
    " - If `0 < n_components < 1` and `svd_solver == ‘full’`, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by `n_components` \n",
    " - `n_components` cannot be equal to `n_features` for `svd_solver == ‘arpack’`.\n",
    " \n",
    "- **`whiten`**:\n",
    "\n",
    "When `True` (`False` by default) the `components_ vectors` are multiplied by the square root of `n_samples` and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.\n",
    "\n",
    "Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.\n",
    "\n",
    "- **`svd_solver`**: (read the papers in Further Reading section to fully understand how this works)\n",
    "\n",
    " - `auto` :\n",
    "The solver is selected by a default policy based on `X.shape` and `n_components`: \n",
    "    - If the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient `‘randomized’` method is enabled. \n",
    "    - Otherwise the exact full SVD is computed and optionally truncated afterwards.\n",
    "\n",
    " - `full` :\n",
    "Run exact full SVD calling the standard `LAPACK` solver via `scipy.linalg.svd` and select the components by postprocessing\n",
    "\n",
    " - `arpack` :\n",
    "Run SVD truncated to `n_components` calling `ARPACK` solver via `scipy.sparse.linalg.svds`. It requires strictly `0 < n_components < X.shape[1]`.\n",
    "\n",
    " - `randomized` :\n",
    "run randomized SVD by the method of Halko et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selected Attributes**\n",
    "\n",
    "- **`components_`** : array, shape `(n_components, n_features)`\n",
    "\n",
    "Principal axes in feature space, representing the projections of maximum variance in the data. The components are sorted by `explained_variance_`.\n",
    "\n",
    "- **`explained_variance_`**: array, shape `(n_components,)`\n",
    "\n",
    "The amount of variance explained by each of the selected components.\n",
    "\n",
    "Equal to `n_components` largest eigenvalues of the covariance matrix of `X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sparsePCA` in sklearn**\n",
    "\n",
    "The `sparsePCA` component seems to have the same interface as the `PCA` component above.\n",
    "\n",
    "Mini-batch sparse PCA (`MiniBatchSparsePCA`) is a variant of SparsePCA that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a linear manifold to approximate a set of $N$ points $x_i\\in\\mathbb{R}^p$, where the $N$ points are approximated by points in $q\\leq p$ dimensions.\n",
    "- By extension of the above, principle components are a useful tool for dimension reduction and compression.\n",
    "- After dimension reduction, sometimes a better clustering is possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation, Metrics and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three ways to visualize and better understand PCA\n",
    "\n",
    "- Display the principle directions, or loadings in the parameters space.\n",
    "\n",
    "- Show the first two components, resulting in a scatterplot. If the first two dimensions of loadings are also shown on the same graph, it is called a 'biplot' as in Stanford online course.\n",
    "\n",
    "- Display data points that attain extreme scroes on the components; see Figure 14.23 in ESL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- ESL, Section 14.5.1, 14.5.4, 14.5.5\n",
    "- [scikit-learn Document 2.5](http://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- Thomas P. Minka: Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604\n",
    "- M. Tipping and C. Bishop, Probabilistic Principal Component Analysis, Journal of the Royal Statistical Society, Series B, 61, Part 3, pp. 611-622\n",
    "- Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) \n",
    "- A randomized algorithm for the decomposition of matrices Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert\n",
    "- Zou, H., Hastie, T. and Tibshirani, R. (2006). Sparse principal component analysis, Journal of Computational and Graphical Statistics 15(2): 265-28."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
