{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest-Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification\n",
    "\n",
    "In k-Nearest-Neighbor classifier, at each query or to-be-predicted point $x$, just find its $k$ nearest neighbors in the sample and cast a majority vote. Here distance is typically the Euclidean distance. For regression, the average is adopted. \n",
    "\n",
    "This is a very simple, model-free algorithm with some nice theoretical result: according to ESL, the famous result by Cover and Hart (1967) shows that asymptotically (i.e. when we have infinite amount of data and zero bias), the error rate of the 1-nearest-neighbor classifier, that pick the classification and the test point at random with the true class probability, is never more than twice the Bayes rate. But of course in practice the condition of this theorem will never be satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants and Generalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Invariant Metrics and Tangent Distance**\n",
    "\n",
    "The motivation of invariant metrics comes from applying kNN to problems such as digit recognition. Presumably, we want a \"3\" to be close to a rotated \"3\". But the usual Euclidean distance between the pixel graphs of \"3\" and a rotated \"3\" can be large. One possible solution is to consider all transforms of the original graph and then compute distance between manifolds of the transforms, but that can be drastically different from the original graph, making, e.g. \"6\" to be close to \"9\". We thus needs a way to define distance for transforms, but only those local enough to the original graph. Comes in the **tangent distance**. The tangent can be computed by estimating the direction vector from small rotations of the image, or by more sophisticated spatial smoothing methods (did not go into detail here). See the graph below for more intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import os.path\n",
    "path = r\"C:\\Users\\Yixian\\Documents\\machine-learning\"\n",
    "file = \"tangent_distance.JPG\"\n",
    "Image(filename = os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brute-force vs. Tree-based**\n",
    "\n",
    "Fast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for $N$ samples in $p$ dimensions, this approach scales as $O[p N^2]$.\n",
    "\n",
    "To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point $A$ is very distant from point $B$, and point $B$ is very close to point $C$, then we know that points $A$ and $C$ are very distant, without having to explicitly calculate their distance. In this way, the computational cost of a nearest neighbors search can be reduced to $O[D N \\log(N)]$ or better. This is a significant improvement over brute-force for large N. In `sklearn` there are two such alternatives: `algorithm = 'kd_tree'` or `algorithm = 'ball_tree'`; see reference under Further reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaptive Nearest Neighbors**\n",
    "\n",
    "The traditional kNN sufferes in high dimension, in that the nearest $k$ neighbors can be very far away in high dimensions. One can mitigate this issue by expanding the neighborhood along the dimensions that does not have class switching by much. The intuition is better summarized in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = \"adaptive_nearest_neighbor_intuition.JPG\"\n",
    "Image(filename = os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve that automatically, the Discriminant Adaptive Nearest-Neighbor (DANN) metric generalize the distance measure at every query point $x_0$:\n",
    "\n",
    "\\begin{align}\n",
    "D(x, x_0) = (x-x_0)^{\\top}\\Sigma(x-x_0),\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "\\Sigma &= W^{-1/2}[W^{-1/2}BW^{-1/2}+\\epsilon I]W^{-1/2}\\\\\n",
    "&= W^{-1/2}[B^{\\ast}+\\epsilon I]W^{-1/2}.\n",
    "\\end{align}\n",
    "Here $W$ is the pooled within-class covariance matrix $\\sum_{k=1}^K\\pi_kW_k$ and $B$ is the between class covariance matrix $\\sum_{k=1}^K\\pi_k(\\bar{x}_k-\\bar{x})(\\bar{x}_k-\\bar{x})^{\\top}$, with $W$ and $B$ computed using only the 50 nearest neighbored around $x_0$. With such defined metric $D$, the neighbor can indeed expand on directions that do not see much of a class change, thereby mitigating the effect of curse-of-dimensionality on nearest-neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "- kNN is model free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages (might refer to the table in Section 10.7 in ESL)\n",
    "\n",
    "- kNN is memory intensive, since it has to store all the observations - there are ways to mitigate this; see the section below.\n",
    "- kNN will suffer in high-dimensional problems, where the k-nearest points are not local any more. This is something that adaptive nearest neighbor seeks to mitigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Other Models\n",
    "\n",
    "- There is close relationship between nearest-neighbor and prototype methods: in 1-nearest-neighbor classfication, each training point is a prototype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "- kNN is often successful where each class has many possible prototypes, and the decision boundary is very irregular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details and Practical Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`KNeighborClassifier` in `sklearn`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5, weights='uniform', p=2, metric='minkowski')\n",
    "neigh.fit(X, y) \n",
    "print(neigh.predict([[1.1]]))\n",
    "print(neigh.predict_proba([[0.9]]))\n",
    "[[ 0.66666667  0.33333333]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selected Parameters**\n",
    "\n",
    "- **`n_neighbors`**:\n",
    "\n",
    "Number of neighbors to use by default for kneighbors queries.\n",
    "\n",
    "- **`weights`**: \n",
    "\n",
    "Weight function used in prediction. Possible values:\n",
    "\n",
    "    - `'uniform'` : uniform weights. All points in each neighborhood are weighted equally.\n",
    "    - `'distance'` : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "    - `[callable]` : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "\n",
    "- **`algorithm`**: \n",
    "\n",
    "Algorithm used to compute the nearest neighbors:\n",
    "\n",
    "    - `'ball_tree'` will use `BallTree`\n",
    "    - `'kd_tree'` will use `KDTree`\n",
    "    - `'brute'` will use a brute-force search.\n",
    "    - `'auto'` will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n",
    "Note: fitting on sparse input will override the setting of this parameter, using brute force.\n",
    "\n",
    "- **`p`**:\n",
    "\n",
    "Power parameter for the Minkowski metric. When `p = 1`, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for `p = 2`. For arbitrary p, minkowski_distance (l_p) is used.\n",
    "\n",
    "- **`metric`**:\n",
    "\n",
    "The distance metric to use for the tree. The default metric is minkowski, and with `p=2` is equivalent to the standard Euclidean metric. See the documentation of the `DistanceMetric` class for a list of available metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selected Methods**\n",
    "\n",
    "- **`kneighbors([X, n_neighbors, return_distance])`**:\tFinds the K-neighbors of a point.\n",
    "\n",
    "- **`kneighbors_graph([X, n_neighbors, mode])`**:\tComputes the (weighted) graph of k-Neighbors for points in `X`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`KNeighborsRegressor` in `sklearn`**\n",
    "\n",
    "The interface is almost identical to that of `KNeighborsClassifier` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation, Metrics and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- ESL Chapters 13.3-13.4\n",
    "- [sklearn Document 1.6](http://scikit-learn.org/stable/modules/neighbors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- ['Multidimensional binary search trees used for associative searching'](https://dl.acm.org/citation.cfm?doid=361002.361007), Bentley, J.L., Communications of the ACM (1975)\n",
    "- ['Five balltree construction algorithms'](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.8209), Omohundro, S.M., International Computer Science Institute Technical Report (1989)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
