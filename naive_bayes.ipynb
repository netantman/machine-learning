{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification\n",
    "\n",
    "Bayes theorem yields:\n",
    "\n",
    "\\begin{align}\n",
    "P(y|x_1,\\dots, x_N) \\propto P(y)P(x_1,\\dots, x_N|y).\n",
    "\\end{align}\n",
    "\n",
    "Under the **Naive Bayes assumption**: $P(x_1,\\dots, x_N|y)=\\prod_{i=1}^NP(x_i|y)$. That is, Naive Bayes handles the curve of dimensionality by simply assuming all variables are independent. Different distributional assumption about $P(x_i|y)$ gives rise to different versions of Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants and Generalizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**\n",
    "\n",
    "The likelihood of the features is assumed to be Gaussian:\n",
    "\n",
    "\\begin{align}\n",
    "P(x_i|y) = \\frac{1}{\\sqrt{2\\pi\\sigma_{iy}^2}}\\exp\\left(-\\frac{(x_i-\\mu_{iy})^2}{2\\sigma_{iy}^2}\\right).\n",
    "\\end{align}\n",
    "The parameters $\\mu_i$ and $\\sigma_i$ are estimated intuitively, by the sample mean and standard deviations of samples belonging the corresonding class $y$, respetively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bernoulli Naive Bayes**\n",
    "\n",
    "Bernoulli Naive Bayes, or **Bernoulli event model** works with **occurence vector**, i.e. $x_i=1$ when the $i$-th word is in the document, and $0$ otherwise. The model is first the document type is generated, and then by some probability each word is going to independently appear or not appear in the document. Parameters are $\\phi_k$, which is the probably that the document is in class $k$ and $\\phi_{i|y=k}$, which is the probability of feature $i$ appearing in the document given that the document belongs to class $k$ ($k=1,\\dots, K$). The smoothed version of the maximum likelihood is just as simple as inpecting the **occurence frequency**:\n",
    "\\begin{align}\n",
    "\\hat{\\phi}_k &= \\frac{\\sum_{n=1}^N1(\\text{the $n$-th document is in class $k$})+\\alpha}{N+K\\alpha}\\\\\n",
    "\\hat{\\phi}_{i|y=k} &=\\frac{\\sum_{n=1}^N1(\\text{the $n$-th document is in class $k$ and it has feature $i$})+\\alpha}{\\sum_{n=1}^N1(\\text{the $n$-th document is in class $k$})+K\\alpha}.\n",
    "\\end{align}\n",
    "The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called **Laplace smoothing**, while $\\alpha < 1$ is called **Lidstone smoothing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Naive Bayes**\n",
    "\n",
    "Multinomial Naive Bayes, or **multinomial event model**, where data is typically represented as **word vector count** (although tf-idf vectors are also known to work well in practice). The model is first the document type is generated, and then each word is indenpendently 'written'. Parameters are $\\phi_k$, which is the probably that the document is in class $k$ and $\\phi_{i|y=k}$, which is the probability of word $i$ appearing as the 'next word' in the document given that the document belongs to class $k$ ($k=1,\\dots, K$). The smoothed version of the maximum likelihood is just inspecting the **counting frequency**.\n",
    "\\begin{align}\n",
    "\\hat{\\phi}_k &= \\frac{\\sum_{n=1}^N1(\\text{the $n$-th document is in class $k$})+\\alpha}{N+K\\alpha}\\\\\n",
    "\\hat{\\phi}_{i|y=k} &=\\frac{\\sum_{n=1}^N\\sum_{j=1}^{m_n}1(\\text{the $n$-th document is in class $k$ and the $j$-th word in the document is $i$})+\\alpha}{\\sum_{n=1}^N1(\\text{the $n$-th document is in class $k$})m_n+|V|\\alpha},\n",
    "\\end{align}\n",
    "where $m_n$ is the total number of words in the $n$-th document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "- The independence assumption of Naive Bayes is (surpringsly) powerful when the dimension $p$ of the feature space is high. In fact, despite these rather optimistic assumptions, naive Bayes classifiers often outperform far more sophisticated alternatives. The intuition is that, although the individual class density estimates may be biased, this bias might not hurt the posterior probabilitieis as much, especially near the decision region. Despite the presence of strong dependence, naive Bayes can still be optimal if the dependences distributes evenly in classes, or if the dependences cancel out each other; see the reference under Further Reading below.\n",
    "- If a component $X_j$ of $X$ is discrete, then an appropriate histogram estimate can be used. This provides a seamless way of mixing variable types in a feature vector.\n",
    "- The Naive Bayes supports 'online learning', or is an 'incremental learner', meaning it can update its model one training example at a time, without re-processing all previous examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "- Due to the over-simplifying assumption of independence, the class probability outputs, e.g. from `predict_proba` in `sklearn` are not to be taken too seriously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to Other Models\n",
    "\n",
    "- Both Naive Bayes and the Generlized Additive Models have divided the joint probabilities by individual marginal probabilities by some sort of independence. But Naive Bayes takes further parametric distributional assumptions on the marginal probabilities. This difference is similar to the difference between [linear discriminant analysis](LDA.ipynb) and [logistic regression](logistic_regression).\n",
    "- Algorithms that try to learn $p(y|x)$ directly are called **discrimative algorithms**, while algorithms which try to model $p(x|y)$ are called **generative learning algorithms**. Naive Bayes is a kind of generative learning algorithms; see more discussion in the [LDA notebook](LDA and QDA.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "- Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details and Practical Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`BernoulliNB` in `sklearn`** (the interface of **`MultinomialNB`** is similar and hence omitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.randint(2, size=(6, 100))\n",
    "Y = np.array([1, 2, 3, 4, 4, 5])\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict(X[2:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "- **`alpha`**:\n",
    "\n",
    "    Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n",
    "    \n",
    "\n",
    "- **`binarize`**:\n",
    "\n",
    "    Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors.\n",
    "    \n",
    "\n",
    "- **`fit_prior`**:\n",
    "\n",
    "    Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n",
    "    \n",
    "\n",
    "- **`class_prior`**:\n",
    "\n",
    "    Prior probabilities of the classes. If specified the priors are not adjusted according to the data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attributes**\n",
    "\n",
    "- **`class_log_prior_`**:\n",
    "\n",
    "    Log probability of each class (smoothed). Not to be trusted with great confidence, per discussed.\n",
    "    \n",
    "\n",
    "- **`feature_log_prob_`**:\n",
    "\n",
    "    Empirical log probability of features given a class, $P(x_i|y)$.\n",
    "    \n",
    "\n",
    "- **`class_count_`**:\n",
    "\n",
    "    Number of samples encountered for each class during fitting. This value is weighted by the sample weight when provided.\n",
    "    \n",
    "\n",
    "- **`feature_count_`**:\n",
    "\n",
    "    Number of samples encountered for each `(class, feature)` during fitting. This value is weighted by the sample weight when provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selected Method**\n",
    "\n",
    "- **`partial_fit(X, y[, classes, sample_weight])`**: Incremental fit on a batch of samples. That is, Naive Bayes models in `sklearn` can be used to tackle large scale classification problems for which the full training set might not fit in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`GaussianNB` in `sklearn`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB(priors=None)\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict([[-0.8, -1]]))\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, Y, np.unique(Y))\n",
    "print(clf_pf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**\n",
    "\n",
    "- **`priors`**: shape `(n_classes,)`\n",
    "\n",
    "    Prior probabilities of the classes. If specified the priors are not adjusted according to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selected Attributes**\n",
    "\n",
    "- **`theta_`**: shape `(n_classes, n_features)`\n",
    "\n",
    "    mean of each feature per class\n",
    "\n",
    "- **`sigma_`**: shape `(n_classes, n_features)`\n",
    "\n",
    "    variance of each feature per class\n",
    "    \n",
    "The methods of `GaussianNB` are similar to the other two NB algorithms in `sklearn`, and hence omitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constructing the Vocabulary or Dictionary**\n",
    "\n",
    "In text classification, it is of practical importance to choose the dictionary by which to construct the feature vector. \n",
    "\n",
    "- Rather than looking through an English dictionary for the list of all English words, it is more common to look through the training set and encode in our feature vector only the words that occur at least once. Apart from reducing the number of words modeled and hence reducing our computational and space requirements, this also has the advantage of allowing us to model/include as a feature many words that may appear in the corpus but that you won't find in a dictionary.\n",
    "- Sometimes we also exclude the very high frequency words, such as 'the', \"of\", \"and\". These high frequency, \"content-free\" words are called **stop words** since they occur in so many documents and do little to indicate what class it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "Naive Bayes works well in problems of text classificiation and spam filtering. Since it is very fastness and is straightforward to implement, it is usually the first to try in such tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Interpretation, Metrics and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- ESL Chapter 6.6.3\n",
    "- [`sklearn` Document 1.9](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- Data Science for Business, Chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "- H. Zhang (2004). [The optimality of Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html). Proc. FLAIRS.\n",
    "- C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265.\n",
    "- A. McCallum and K. Nigam (1998). [A comparison of event models for Naive Bayes text classification.](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1529) Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.\n",
    "- V. Metsis, I. Androutsopoulos and G. Paliouras (2006). [Spam filtering with Naive Bayes – Which Naive Bayes?](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.5542) 3rd Conf. on Email and Anti-Spam (CEAS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
